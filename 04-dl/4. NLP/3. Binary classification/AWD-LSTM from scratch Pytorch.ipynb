{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: We need preprocessed texts (tokinized and numericalized) in this notebook\n",
    "\n",
    "> Notebook based on:\n",
    "> 1. https://github.com/fastai/course-v3/blob/master/nbs/dl2/12_text.ipynb\n",
    "> 2. https://github.com/fastai/course-v3/blob/master/nbs/dl2/12a_awd_lstm.ipynb\n",
    "> 3. https://github.com/fastai/course-v3/blob/master/nbs/dl2/12b_lm_pretrain.ipynb\n",
    "> 4. https://github.com/fastai/course-v3/blob/master/nbs/dl2/12c_ulmfit.ipynb\n",
    "> \n",
    "> Video:\n",
    "> - https://youtu.be/vnOpEwmtFJ8?t=4687 from 1:18:00 to 2:08:00 (50 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pathlib\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test  train  unsup  vocab.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls \"../../Datasets/NLP/IMBd_prepro\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg  pos\r\n"
     ]
    }
   ],
   "source": [
    "!ls \"../../Datasets/NLP/IMBd_prepro/train\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Dataset & Dataloader for classification\n",
    "\n",
    "- **Dataset**\n",
    "  - **X**: Some unique text (a review in IMDb dataset)\n",
    "  - **Y**: Some class label (pos or neg in IMDb dataset)\n",
    "- **Dataloader**\n",
    "  - Batch: We need **padding** for dealing with texts of different lenghts.\n",
    "  - Sampler: To avoid mixing very long texts with very short ones, we will also use `Sampler` to sort (with a bit of randomness for the training set) our samples by length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FolderDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, root_dir, file_extensions, x_tfms=None):\n",
    "        root_dir = pathlib.Path(root_dir)\n",
    "\n",
    "        def dir_conditions(folder: pathlib.PosixPath) -> bool:\n",
    "            is_folder  = folder.is_dir()\n",
    "            not_empty  = any(folder.iterdir()) if is_folder else False\n",
    "            not_hidden = not folder.name.startswith('.')\n",
    "            return is_folder and not_empty and not_hidden\n",
    "        \n",
    "        def file_conditions(file: pathlib.PosixPath) -> bool:\n",
    "            is_file    = file.is_file()\n",
    "            good_file  = file.name.endswith(file_extensions) # str or tuple of strings\n",
    "            not_hidden = not file.name.startswith('.')\n",
    "            return is_file and good_file and not_hidden\n",
    "\n",
    "        #               __element__  ___________________________loop___________________________   \n",
    "        self.classes = [folder.name  for folder in root_dir.iterdir() if dir_conditions(folder)]\n",
    "        self.classes.sort()\n",
    "        \n",
    "        self.class2num = {cls_name: i for i,cls_name in enumerate(self.classes)}\n",
    "\n",
    "        #                 ______________xy_element______________    \n",
    "        self.samples = [ (str(file), self.class2num[folder.name]) \n",
    "        #                _________________________1st loop_________________________\n",
    "                         for folder in root_dir.iterdir() if dir_conditions(folder)\n",
    "        #                ______________________2nd loop______________________\n",
    "                         for file in folder.iterdir() if file_conditions(file) ]\n",
    "        \n",
    "        self.x_tfms = x_tfms\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        x, y = self.samples[idx]\n",
    "        if self.x_tfms: x = self.x_tfms(x)\n",
    "        \n",
    "        return torch.tensor(x), torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = FolderDataset(root_dir        = '../../Datasets/NLP/IMBd_prepro/train',\n",
    "                         file_extensions = (\".npy\"),\n",
    "                         x_tfms          = lambda path: np.load(path).astype(\"int64\") )\n",
    "\n",
    "ds_valid = FolderDataset(root_dir        = '../../Datasets/NLP/IMBd_prepro/test',\n",
    "                         file_extensions = (\".npy\"),\n",
    "                         x_tfms          = lambda path: np.load(path).astype(\"int64\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([    2,     7,    64,    12,  1405,    29,    15,    19,    18,    41,\n",
       "            37,   690,    14,  1690,     9,    18,    58,    35,   140,   109,\n",
       "           234,     8,   706,    13,     8,    29,    15, 31838,    16,   155,\n",
       "            42, 50903,    30,  3837,    74,    43,   345,    78,   114,    19,\n",
       "            29,    11,     8,   137,    47,     7,  8030,    15,    56,  1405,\n",
       "             8,   348,   331,    13,     8,    29,   181,  2388,  1532,    11,\n",
       "             8,    29,    61,    12,    83,    67,  2102,   291,    80,   181,\n",
       "             8,    29,   429,     9,    32,   155,   126,    19,    29,   301,\n",
       "            10,    18,   122,  1559,    88,    16,    22,    72,    14, 19687,\n",
       "             7,  3837,    18,   133,    19,    29,    11,    16,    22,    37,\n",
       "            56,   635,    11,   172,    30,    16,   103,   426,  2388,    12,\n",
       "            83,    67,  9245,    11,    64,  2388,   155,    42,   422,    52,\n",
       "            18,   122,    19,    15,    43,    13,     8,   139,    29,    17,\n",
       "             8,  2699,   498,     9,    30,    18,   140,     8,   100,    61,\n",
       "             0,  5018,    52,  3918,     9,     8,    81,   169,    18,    58,\n",
       "            35,    53,    59,    19,    29,    15,     8,   238,    88,     8,\n",
       "           948,   238,    15,    67,    30,     8,   779,    38,    37,    67,\n",
       "           214,    18,   122,     8,   238,    73,    42,   146,    63,    12,\n",
       "          2082,     7, 18344,    73,    42,     8,   238,   170,    30,  3837,\n",
       "            90,   197,    35,    58,   255,    59,    20,    52,  3918,     9,\n",
       "             3]),\n",
       " tensor(1))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([    2,     7,   365,    10,   740,    10,    47,    23,     8,    23,\n",
       "          1637,   201,  5532,    45,   242,    61,   206,    67,   326,   306,\n",
       "            11,   983,    13,    36,  2952,    33,  4889,    11,   614,    14,\n",
       "          4807,    36,  2952,    33,   502,   467,     9,     7,   676,    16,\n",
       "            22,    59,  1342,  4954, 20939,    34,    12,   288,    13,   121,\n",
       "            90,    58,    35,   479,    48, 15666,    59,    23,    63,    20,\n",
       "            22,   147,   169,    10,  3085,    17,     9,    36,   194,   383,\n",
       "           124,   263,    33,     3]),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader with Custom sampler and collate_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samplers for putting texts of similar lenght together\n",
    "- **For the validation set**: we will simply sort the samples by length, and we begin with the longest ones for memory reasons (it's better to always have the biggest tensors first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampler for validation\n",
    "class SortSampler(Sampler):\n",
    "    \n",
    "    def __init__(self, data_source, key):\n",
    "        self.data_source = data_source\n",
    "        self.key         = key\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_source)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(sorted(list(range(len(self.data_source))), key=self.key, reverse=True))\n",
    "\n",
    "# Sampler for training\n",
    "class SortishSampler(Sampler):\n",
    "    def __init__(self, data_source, key, bs):\n",
    "        self.data_source,self.key,self.bs = data_source,key,bs\n",
    "\n",
    "    def __len__(self) -> int: return len(self.data_source)\n",
    "\n",
    "    def __iter__(self):\n",
    "        idxs = torch.randperm(len(self.data_source))\n",
    "        megabatches = [idxs[i:i+self.bs*50] for i in range(0, len(idxs), self.bs*50)]\n",
    "        sorted_idx = torch.cat([tensor(sorted(s, key=self.key, reverse=True)) for s in megabatches])\n",
    "        batches = [sorted_idx[i:i+self.bs] for i in range(0, len(sorted_idx), self.bs)]\n",
    "        max_idx = torch.argmax(tensor([self.key(ck[0]) for ck in batches]))  # find the chunk with the largest key,\n",
    "        batches[0],batches[max_idx] = batches[max_idx],batches[0]            # then make sure it goes first.\n",
    "        batch_idxs = torch.randperm(len(batches)-2)\n",
    "        sorted_idx = torch.cat([batches[i+1] for i in batch_idxs]) if len(batches) > 1 else LongTensor([])\n",
    "        sorted_idx = torch.cat([batches[0], sorted_idx, batches[-1]])\n",
    "        return iter(sorted_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Collate for adding PADDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor([0]),\n",
       "  tensor([8, 0]),\n",
       "  tensor([1, 1, 8]),\n",
       "  tensor([2, 3, 6, 9]),\n",
       "  tensor([2, 8, 5, 5, 1]),\n",
       "  tensor([3, 4, 2, 1, 6, 1]),\n",
       "  tensor([4, 3, 4, 0, 3, 3, 3]),\n",
       "  tensor([9, 9, 5, 9, 1, 2, 7, 7]),\n",
       "  tensor([7, 1, 5, 3, 0, 5, 9, 7, 1]),\n",
       "  tensor([0, 6, 6, 5, 9, 9, 4, 8, 1, 2])],\n",
       " [10, 11, 12, 13, 14, 15, 16, 17, 18, 19])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = list([torch.randint(0, 10, (x,)) for x in range(1, 11)])\n",
    "y = list(range(10,20))\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3, 4, 2, 1, 6, 1]), 15)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = list(zip(x,y))\n",
    "dataset[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 6, 6, 5, 9, 9, 4, 8, 1, 2],\n",
      "        [8, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [9, 9, 5, 9, 1, 2, 7, 7, 0, 0]])\n",
      "tensor([[3, 4, 2, 1, 6, 1],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 8, 0, 0, 0]])\n",
      "tensor([[7, 1, 5, 3, 0, 5, 9, 7, 1],\n",
      "        [2, 8, 5, 5, 1, 0, 0, 0, 0],\n",
      "        [4, 3, 4, 0, 3, 3, 3, 0, 0]])\n",
      "tensor([[2, 3, 6, 9]])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def pad_collate(batch):\n",
    "    # batch looks like [(x0,y0), (x4,y4), (x2,y2)... ]\n",
    "    batch_x, batch_y = zip(*batch) \n",
    "    \n",
    "    batch_x = pad_sequence(batch_x, batch_first=True, padding_value=1)\n",
    "    batch_y = torch.tensor(batch_y)\n",
    "    \n",
    "    return batch_x, batch_y\n",
    "\n",
    "dl = torch.utils.data.DataLoader(dataset,\n",
    "                                 batch_size=3,\n",
    "                                 shuffle=True,\n",
    "                                 collate_fn=pad_collate)\n",
    "for xb, yb in dl:\n",
    "    print(xb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
