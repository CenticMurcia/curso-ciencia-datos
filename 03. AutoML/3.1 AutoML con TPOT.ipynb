{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NwMuduTDiZP_"
   },
   "source": [
    "# TPOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8H0ktRvDimAU"
   },
   "source": [
    "Fuente: https://raw.githubusercontent.com/rpi-techfundamentals/fall2018-materials/master/fig/final-logo.png)](http://rpi.analyticsdojo.com\n",
    "https://towardsdatascience.com/automated-machine-learning-on-the-cloud-in-python-47cf568859f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rbiKVIwfHxxS"
   },
   "source": [
    "# Introduction: Automated Machine Learning\n",
    "\n",
    "En este Notebook, veremos cómo usar [TPOT] (https://epistasislab.github.io/tpot/api/), una libreria de Python que permite el preprocesamiento automático, selección de modelos y ajuste de hiperparámetros. Utilizando [programación genética] (http://geneticprogramming.com/tutorial/), TPOT intenta encontrar la mejor canalización de aprendizaje automático para un conjunto de datos evaluando miles de posibilidades.\n",
    "\n",
    "Los pasos de aprendizaje automático en este contexto consisten en:\n",
    "\n",
    "1. Preprocesamiento de funciones\n",
    "  * Imputar valores perdidos y escalar valores\n",
    "  * Construir nuevas características como transformaciones polinomiales\n",
    "2. Selección de funciones\n",
    "  * Reducción de dimensionalidad, por ejemplo usando PCA y otras técnicas\n",
    "3. Selección de modelo\n",
    "  * Evaluación de varios modelos de aprendizaje automático\n",
    "4. Ajuste de hiperparámetros\n",
    "  * Encontrar la configuración óptima del modelo para el problema particular\n",
    "\n",
    "TPOT pertenece a una clase de métodos conocidos como [auto-ml (abreviatura de aprendizaje automático automatizado)] (https://www.kdnuggets.com/2017/01/current-state-automated-machine-learning.html) que tienen como objetivo simplificar el trabajo del científico de datos al encontrar automáticamente los pasos de preprocesamiento de características óptimos y el modelo. \n",
    "\n",
    "El aprendizaje automático suele ser una parte de un problema de ciencia de datos que requiere mucho tiempo y mucho conocimiento. Auto-ml no está diseñado para reemplazar al científico de datos, sino para liberarlo para trabajar en aspectos más importantes del problema completo, como adquirir datos e interpretar los resultados del modelo. \n",
    "\n",
    "Otras fuentes de modelos de AutoML:\n",
    "\n",
    "* [Auto-sklearn](https://automl.github.io/auto-sklearn/stable/)\n",
    "* [H20](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html)\n",
    "* [Google Cloud AutoML](https://cloud.google.com/automl/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eXV0OhoZylj8"
   },
   "source": [
    "## Instalación de Tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593
    },
    "colab_type": "code",
    "id": "UX2YHrGwOuUN",
    "outputId": "84270bc5-5f8a-4f90-92da-191e28a3bb2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tpot in c:\\users\\leyre\\anaconda3\\lib\\site-packages (0.11.5)\n",
      "Requirement already satisfied: tqdm>=4.36.1 in c:\\users\\leyre\\anaconda3\\lib\\site-packages (from tpot) (4.42.1)\n",
      "Requirement already satisfied: stopit>=1.1.1 in c:\\users\\leyre\\anaconda3\\lib\\site-packages (from tpot) (1.1.2)\n",
      "Requirement already satisfied: pandas>=0.24.2 in c:\\users\\leyre\\anaconda3\\lib\\site-packages (from tpot) (1.0.1)\n",
      "Requirement already satisfied: scipy>=1.3.1 in c:\\users\\leyre\\anaconda3\\lib\\site-packages (from tpot) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\users\\leyre\\anaconda3\\lib\\site-packages (from tpot) (0.14.1)\n",
      "Requirement already satisfied: deap>=1.2 in c:\\users\\leyre\\anaconda3\\lib\\site-packages (from tpot) (1.3.1)\n",
      "Requirement already satisfied: update-checker>=0.16 in c:\\users\\leyre\\anaconda3\\lib\\site-packages (from tpot) (0.18.0)\n",
      "Requirement already satisfied: scikit-learn>=0.22.0 in c:\\users\\leyre\\anaconda3\\lib\\site-packages (from tpot) (0.22)\n",
      "Requirement already satisfied: numpy>=1.16.3 in c:\\users\\leyre\\anaconda3\\lib\\site-packages (from tpot) (1.18.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\leyre\\anaconda3\\lib\\site-packages (from pandas>=0.24.2->tpot) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\leyre\\anaconda3\\lib\\site-packages (from pandas>=0.24.2->tpot) (2.8.1)\n",
      "Requirement already satisfied: requests>=2.3.0 in c:\\users\\leyre\\anaconda3\\lib\\site-packages (from update-checker>=0.16->tpot) (2.22.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\leyre\\anaconda3\\lib\\site-packages (from python-dateutil>=2.6.1->pandas>=0.24.2->tpot) (1.14.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\leyre\\anaconda3\\lib\\site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (1.25.8)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\leyre\\anaconda3\\lib\\site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\leyre\\anaconda3\\lib\\site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2020.6.20)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\leyre\\anaconda3\\lib\\site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "# Install tpot on the server\n",
    "!pip install tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "colab_type": "code",
    "id": "uhBlHuGpO2n6",
    "outputId": "4c7cd3ca-b611-49fd-94dc-6c928be7cafb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leyre\\anaconda3\\lib\\site-packages\\tpot\\builtins\\__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n"
     ]
    }
   ],
   "source": [
    "# pandas and numpy for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import the tpot regressor\n",
    "from tpot import TPOTRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rBJofhDZzuG3"
   },
   "source": [
    "# Descripción del problema\n",
    "\n",
    "Se trata de un problema de regresión supervisada: datos [datos de energía de la ciudad de Nueva York] (http://www.nyc.gov/html/gbee/html/plan/ll84_scores.shtml), queremos construir un modelo que pueda predecir el Energy Star Score de un edificio. Utilizaremos la ingeniería de características manual, la reducción de dimensionalidad, la selección del modelo y el ajuste de hiperparámetros pra tal fin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dMni42UjIs75"
   },
   "source": [
    "## Conjunto de datos\n",
    "\n",
    "Las características contienen una serie de variables numéricas continuas (como el uso de energía y el área del edificio), así como dos variables categóricas codificadas en caliente (municipio y tipo de edificio). Hay un total de 82 funciones.\n",
    "\n",
    "Todos los valores perdidos se han codificado como \"np.nan\" y TPOT realizará automáticamente la imputación del valor perdido. También escala automáticamente las variables para que no tengamos que preocuparnos por normalizar el rango de cada característica. TPOT realiza tanto ingeniería de características como selección de características, por lo que no transformaremos ninguna de las variables ni eliminaremos características extrañas que pensamos que pueden ser extrañas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "4O5zosPFO4U5",
    "outputId": "b13543f6-19b5-4328-e963-6ed54fc3a83f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape:  (6622, 82)\n",
      "Testing features shape:   (2839, 82)\n"
     ]
    }
   ],
   "source": [
    "# Read in features from GitHub\n",
    "train_features = pd.read_csv('https://raw.githubusercontent.com/WillKoehrsen/machine-learning-project-walkthrough/master/data/X_train.csv')\n",
    "test_features = pd.read_csv('https://raw.githubusercontent.com/WillKoehrsen/machine-learning-project-walkthrough/master/data/X_test.csv')\n",
    "\n",
    "# Read in labels from GitHub\n",
    "train_labels = pd.read_csv('https://raw.githubusercontent.com/WillKoehrsen/machine-learning-project-walkthrough/master/data/Y_train.csv')\n",
    "test_labels = pd.read_csv('https://raw.githubusercontent.com/WillKoehrsen/machine-learning-project-walkthrough/master/data/Y_test.csv')\n",
    "\n",
    "print('Training features shape: ', train_features.shape)\n",
    "print('Testing features shape:  ', test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "colab_type": "code",
    "id": "Lrr9diIv1izQ",
    "outputId": "eab72970-7599-48a1-eb50-d7838818225c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order</th>\n",
       "      <th>Property Id</th>\n",
       "      <th>DOF Gross Floor Area</th>\n",
       "      <th>Largest Property Use Type - Gross Floor Area (ft²)</th>\n",
       "      <th>Year Built</th>\n",
       "      <th>Number of Buildings - Self-reported</th>\n",
       "      <th>Occupancy</th>\n",
       "      <th>Site EUI (kBtu/ft²)</th>\n",
       "      <th>Weather Normalized Site EUI (kBtu/ft²)</th>\n",
       "      <th>Weather Normalized Site Electricity Intensity (kWh/ft²)</th>\n",
       "      <th>...</th>\n",
       "      <th>Largest Property Use Type_Restaurant</th>\n",
       "      <th>Largest Property Use Type_Retail Store</th>\n",
       "      <th>Largest Property Use Type_Self-Storage Facility</th>\n",
       "      <th>Largest Property Use Type_Senior Care Community</th>\n",
       "      <th>Largest Property Use Type_Social/Meeting Hall</th>\n",
       "      <th>Largest Property Use Type_Strip Mall</th>\n",
       "      <th>Largest Property Use Type_Supermarket/Grocery Store</th>\n",
       "      <th>Largest Property Use Type_Urgent Care/Clinic/Other Outpatient</th>\n",
       "      <th>Largest Property Use Type_Wholesale Club/Supercenter</th>\n",
       "      <th>Largest Property Use Type_Worship Facility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13276</td>\n",
       "      <td>5849784</td>\n",
       "      <td>90300.0</td>\n",
       "      <td>77300.0</td>\n",
       "      <td>1950</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>126.0</td>\n",
       "      <td>136.8</td>\n",
       "      <td>5.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7377</td>\n",
       "      <td>4398442</td>\n",
       "      <td>52000.0</td>\n",
       "      <td>52000.0</td>\n",
       "      <td>1926</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>95.4</td>\n",
       "      <td>102.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9479</td>\n",
       "      <td>4665374</td>\n",
       "      <td>104700.0</td>\n",
       "      <td>105000.0</td>\n",
       "      <td>1954</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>40.4</td>\n",
       "      <td>40.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14774</td>\n",
       "      <td>3393340</td>\n",
       "      <td>129333.0</td>\n",
       "      <td>129333.0</td>\n",
       "      <td>1992</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>157.1</td>\n",
       "      <td>163.1</td>\n",
       "      <td>16.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3286</td>\n",
       "      <td>2704325</td>\n",
       "      <td>109896.0</td>\n",
       "      <td>116041.0</td>\n",
       "      <td>1927</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>62.3</td>\n",
       "      <td>68.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Order  Property Id  DOF Gross Floor Area  \\\n",
       "0  13276      5849784               90300.0   \n",
       "1   7377      4398442               52000.0   \n",
       "2   9479      4665374              104700.0   \n",
       "3  14774      3393340              129333.0   \n",
       "4   3286      2704325              109896.0   \n",
       "\n",
       "   Largest Property Use Type - Gross Floor Area (ft²)  Year Built  \\\n",
       "0                                            77300.0         1950   \n",
       "1                                            52000.0         1926   \n",
       "2                                           105000.0         1954   \n",
       "3                                           129333.0         1992   \n",
       "4                                           116041.0         1927   \n",
       "\n",
       "   Number of Buildings - Self-reported  Occupancy  Site EUI (kBtu/ft²)  \\\n",
       "0                                    1        100                126.0   \n",
       "1                                    1        100                 95.4   \n",
       "2                                    1        100                 40.4   \n",
       "3                                    1        100                157.1   \n",
       "4                                    1        100                 62.3   \n",
       "\n",
       "   Weather Normalized Site EUI (kBtu/ft²)  \\\n",
       "0                                   136.8   \n",
       "1                                   102.0   \n",
       "2                                    40.0   \n",
       "3                                   163.1   \n",
       "4                                    68.2   \n",
       "\n",
       "   Weather Normalized Site Electricity Intensity (kWh/ft²)  ...  \\\n",
       "0                                                5.2        ...   \n",
       "1                                                4.7        ...   \n",
       "2                                                3.8        ...   \n",
       "3                                               16.9        ...   \n",
       "4                                                3.5        ...   \n",
       "\n",
       "   Largest Property Use Type_Restaurant  \\\n",
       "0                                     0   \n",
       "1                                     0   \n",
       "2                                     0   \n",
       "3                                     0   \n",
       "4                                     0   \n",
       "\n",
       "   Largest Property Use Type_Retail Store  \\\n",
       "0                                       0   \n",
       "1                                       0   \n",
       "2                                       0   \n",
       "3                                       0   \n",
       "4                                       0   \n",
       "\n",
       "   Largest Property Use Type_Self-Storage Facility  \\\n",
       "0                                                0   \n",
       "1                                                0   \n",
       "2                                                0   \n",
       "3                                                0   \n",
       "4                                                0   \n",
       "\n",
       "   Largest Property Use Type_Senior Care Community  \\\n",
       "0                                                0   \n",
       "1                                                0   \n",
       "2                                                0   \n",
       "3                                                1   \n",
       "4                                                0   \n",
       "\n",
       "   Largest Property Use Type_Social/Meeting Hall  \\\n",
       "0                                              0   \n",
       "1                                              0   \n",
       "2                                              0   \n",
       "3                                              0   \n",
       "4                                              0   \n",
       "\n",
       "   Largest Property Use Type_Strip Mall  \\\n",
       "0                                     0   \n",
       "1                                     0   \n",
       "2                                     0   \n",
       "3                                     0   \n",
       "4                                     0   \n",
       "\n",
       "   Largest Property Use Type_Supermarket/Grocery Store  \\\n",
       "0                                                  0     \n",
       "1                                                  0     \n",
       "2                                                  0     \n",
       "3                                                  0     \n",
       "4                                                  0     \n",
       "\n",
       "   Largest Property Use Type_Urgent Care/Clinic/Other Outpatient  \\\n",
       "0                                                  0               \n",
       "1                                                  0               \n",
       "2                                                  0               \n",
       "3                                                  0               \n",
       "4                                                  0               \n",
       "\n",
       "   Largest Property Use Type_Wholesale Club/Supercenter  \\\n",
       "0                                                  0      \n",
       "1                                                  0      \n",
       "2                                                  0      \n",
       "3                                                  0      \n",
       "4                                                  0      \n",
       "\n",
       "   Largest Property Use Type_Worship Facility  \n",
       "0                                           0  \n",
       "1                                           0  \n",
       "2                                           0  \n",
       "3                                           0  \n",
       "4                                           0  \n",
       "\n",
       "[5 rows x 82 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lhbd6f_bKKgb"
   },
   "source": [
    "En el siguiente código, convertimos a matrices numpy. Esto no es estrictamente necesario, pero las etiquetas deben convertirse a un vector unidimensional (usando remodelar en el código a continuación) o Scikit-Learn mostrará un mensaje de advertencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Txom0ypdPE4f"
   },
   "outputs": [],
   "source": [
    "# Convert to numpy arrays\n",
    "training_features = np.array(train_features)\n",
    "testing_features = np.array(test_features)\n",
    "\n",
    "# Sklearn wants the labels as one-dimensional vectors\n",
    "training_targets = np.array(train_labels).reshape((-1,))\n",
    "testing_targets = np.array(test_labels).reshape((-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OLUqRxbnLp6h"
   },
   "source": [
    "Después de la preparación mínima de datos, podemos crear el optimizador TPOT. La sintaxis de los optimizadores TPOT (https://epistasislab.github.io/tpot/using/#tpot-with-code) está diseñada para ser lo más cercana posible a la de los modelos Scikit-Learn.\n",
    "\n",
    "Los [parámetros predeterminados para los optimizadores de TPOT] (https://epistasislab.github.io/tpot/api/) probarán 100 poblaciones de tuberías, cada una con 100 generaciones para un total de 10,000 tuberías. Con una validación cruzada de 10 veces, esto representa 100.000 ejecuciones de entrenamiento. Esto lleva bastante tiempo. Para evitar quedarse sin tiempo estableceremos un máximo de 15 minutos para la evaluación. [TPOT está diseñado para ejecutarse durante días] (https://epistasislab.github.io/tpot/using/) para evaluar a fondo muchas canalizaciones, pero los resultados pueden ser bastante buenos incluso con unas pocas horas de entrenamiento.\n",
    "\n",
    "Establecemos los siguientes parámetros en la llamada al optimizador (no dude en cambiarlos y ver cómo afectan los resultados):\n",
    "\n",
    "* `scoring = neg_mean_absolute_error`: Nuestra métrica de rendimiento de regresión seleccionada\n",
    "* `max_time_mins = 15`: Limite la evaluación a 15 minutos\n",
    "* `n_jobs = -1`: Usa todos los núcleos disponibles en la máquina\n",
    "* `verbosity = 2`: muestra una cantidad limitada de información durante el entrenamiento\n",
    "* `cv = 5`: Utilice una validación cruzada de 5 veces (el valor predeterminado es 10)\n",
    "\n",
    "Después de crear el optimizador, lo \"ajustamos\" a los datos de entrenamiento como con cualquier modelo de aprendizaje automático de Scikit-Learn. Esto inicia el proceso de optimización que continuará durante 8 horas. Durante el entrenamiento, podemos ver una cantidad limitada de información (cambie la \"verbosidad\" para ver más o menos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g268joSvPGMl"
   },
   "outputs": [],
   "source": [
    "# Create a tpot object with a few parameters\n",
    "tpot = TPOTRegressor(scoring = 'neg_mean_absolute_error', \n",
    "                    max_time_mins = 15, \n",
    "                    n_jobs = -1,\n",
    "                    verbosity = 2,\n",
    "                    cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112,
     "referenced_widgets": [
      "c74dab0135534529a15c306e6cc5c2f8"
     ]
    },
    "colab_type": "code",
    "id": "17SbSKs-Qg2-",
    "outputId": "82284432-2d32-44b9-dc72-599d8ab1a203"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing missing values in feature set\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65880cfe3fc74526a816d6c3125ef68c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Optimization Progress', style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: -8.70977287835271\n",
      "Generation 2 - Current best internal CV score: -8.70977287835271\n",
      "15.21 minutes have elapsed. TPOT will close down.\n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "\n",
      "Best pipeline: XGBRegressor(LassoLarsCV(input_matrix, normalize=True), learning_rate=0.1, max_depth=5, min_child_weight=19, n_estimators=100, nthread=1, objective=reg:squarederror, subsample=0.5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TPOTRegressor(config_dict=None, crossover_rate=0.1, cv=5,\n",
       "              disable_update_check=False, early_stop=None, generations=100,\n",
       "              log_file=<ipykernel.iostream.OutStream object at 0x000002DE9B11F9C8>,\n",
       "              max_eval_time_mins=5, max_time_mins=15, memory=None,\n",
       "              mutation_rate=0.9, n_jobs=-1, offspring_size=None,\n",
       "              periodic_checkpoint_folder=None, population_size=100,\n",
       "              random_state=None, scoring='neg_mean_absolute_error',\n",
       "              subsample=1.0, template=None, use_dask=False, verbosity=2,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the tpot model on the training data\n",
    "tpot.fit(training_features, training_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I_Er00uxNa_n"
   },
   "source": [
    "Debido al límite de tiempo, podemos ver que nuestro modelo solo pudo pasar 15 generaciones. Con 100 poblaciones, esto representa 1500 tuberías individuales diferentes que fueron evaluadas, ¡deje algunas más de las que podríamos probar a mano!\n",
    "\n",
    "Una vez que el modelo ha terminado de entrenarse, podemos ver la canalización óptima imprimiendo la \"tubería_ ajustada\". Esto representa la canalización completa con la mejor métrica de rendimiento (en este caso, el `neg_mean_absolute_error` más alto) de la validación cruzada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1P1r23sqQkbW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "         steps=[('stackingestimator',\n",
      "                 StackingEstimator(estimator=LassoLarsCV(copy_X=True, cv=None,\n",
      "                                                         eps=2.220446049250313e-16,\n",
      "                                                         fit_intercept=True,\n",
      "                                                         max_iter=500,\n",
      "                                                         max_n_alphas=1000,\n",
      "                                                         n_jobs=None,\n",
      "                                                         normalize=True,\n",
      "                                                         positive=False,\n",
      "                                                         precompute='auto',\n",
      "                                                         verbose=False))),\n",
      "                ('xgbregressor',\n",
      "                 XGBRegressor(base_score=0.5, booster='gbtree',\n",
      "                              colsample_bylevel=1, colsample_bynode=1,\n",
      "                              colsample_bytree=1, gamma=0,\n",
      "                              importance_type='gain', learning_rate=0.1,\n",
      "                              max_delta_step=0, max_depth=5,\n",
      "                              min_child_weight=19, missing=None,\n",
      "                              n_estimators=100, n_jobs=1, nthread=1,\n",
      "                              objective='reg:squarederror', random_state=0,\n",
      "                              reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "                              seed=None, silent=None, subsample=0.5,\n",
      "                              verbosity=1))],\n",
      "         verbose=False)\n"
     ]
    }
   ],
   "source": [
    "# Show the final model\n",
    "print(tpot.fitted_pipeline_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2jwXtE8s4l6w"
   },
   "source": [
    "El proceso de optimización de TPOT es estocástico, lo que significa que [cada ejecución producirá resultados diferentes] (https://epistasislab.github.io/tpot/using/). Si vuelve a ejecutar este cuaderno, no se preocupe si ve una canalización final diferente.\n",
    "\n",
    "Para guardar la canalización para uso futuro, podemos exportarla a una secuencia de comandos de Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2c9_uFYYGwB8"
   },
   "outputs": [],
   "source": [
    "# Export the pipeline as a python script file\n",
    "tpot.export('tpot_exported_pipeline.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E-y6Qfl58_A_"
   },
   "source": [
    "Si queremos ver todas las canalizaciones evaluadas, podemos ver el atributo `.evaluate_individuals_` del optimizador ajustado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2fkGLxUT9KjK"
   },
   "outputs": [],
   "source": [
    "# To examine all fitted models\n",
    "tpot.evaluated_individuals_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OQPhr6O5N_7h"
   },
   "source": [
    "Finalmente, probemos toda la tubería ajustada en el conjunto de datos de prueba. Después de evaluar todos los pipelines, TPOT guarda el mejor y lo entrena con todos los datos de entrenamiento, para que podamos evaluar el mejor utilizando el método optimizador .score. Esto mostrará el error cuadrático medio negativo, nuestra métrica de regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QAkaMFukQswd"
   },
   "outputs": [],
   "source": [
    "# Evaluate the final model\n",
    "print(tpot.score(testing_features, testing_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para guardar el codigo\n",
    "# Download the pipeline for local use\n",
    "files.download('tpot_exported_pipeline.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí está el codigo final del modelo, del archivo guardado de Python descargado. ¡Podemos entrenarlo y probarlo solo para asegurarnos de que esta puntuación sea correcta!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XVM3v_s66A1b"
   },
   "outputs": [],
   "source": [
    "# Imports that the final pipeline needs\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import LassoLarsCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import Imputer\n",
    "from tpot.builtins import StackingEstimator\n",
    "\n",
    "# Preprocessing steps\n",
    "imputer = Imputer(strategy=\"median\")\n",
    "imputer.fit(training_features)\n",
    "training_features = imputer.transform(training_features)\n",
    "testing_features = imputer.transform(testing_features)\n",
    "\n",
    "# Final pipeline from TPOT\n",
    "exported_pipeline = make_pipeline(\n",
    "    StackingEstimator(estimator=LassoLarsCV(normalize=True)),\n",
    "    GradientBoostingRegressor(alpha=0.95, learning_rate=0.1, loss=\"lad\", \n",
    "                              max_depth=7, max_features=0.75, \n",
    "                              min_samples_leaf=3, min_samples_split=18, \n",
    "                              n_estimators=100, subsample=0.60)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zR_UtoCY7LWr"
   },
   "outputs": [],
   "source": [
    "# Fit on the training data\n",
    "exported_pipeline.fit(training_features, training_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mfge49CA72wc"
   },
   "source": [
    "Después de crear la canalización optimizada y entrenarla, podemos evaluarla en el conjunto de pruebas. Como los modelos no se crearon con un \"estado_aleatorio\", esperamos un rendimiento ligeramente diferente al de los resultados originales, pero debería ser bastante similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WaA26pr87M_A"
   },
   "outputs": [],
   "source": [
    "# Make predictions on the testing data\n",
    "predictions = exported_pipeline.predict(testing_features)\n",
    "\n",
    "print('Mean Absolute Error = %0.4f' % np.mean(abs(predictions - testing_targets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mlab54GV8JTa"
   },
   "source": [
    "Efectivamente, el error absoluto medio está cerca del del método optimizador `.score` y considerablemente mejor que nuestros esfuerzos manuales de construcción de canalizaciones.\n",
    "\n",
    "A partir de aquí, podemos usar los resultados de optimización e intentar ajustar aún más la canalización, o podemos pasar a fases importantes del flujo de trabajo de ciencia de datos. Si usamos esto como modelo final, podríamos dedicar tiempo a tratar de interpretar el modelo (tal vez usando [LIME: Local Interpretable Model-Agnostic Explainations] (https://www.oreilly.com/learning/introduction-to-local- interpretable-model-agnostic-explications-lime)) o informando nuestros resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hvGlKNHMXZOv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "auto_ml_tpot.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
