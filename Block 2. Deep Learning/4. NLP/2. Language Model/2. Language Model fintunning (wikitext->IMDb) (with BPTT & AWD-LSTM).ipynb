{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: We need preprocessed texts (tokinized and numericalized) in this notebook\n",
    "\n",
    "> Notebook based on:\n",
    "> 1. https://github.com/fastai/course-v3/blob/master/nbs/dl2/12_text.ipynb\n",
    "> 2. https://github.com/fastai/course-v3/blob/master/nbs/dl2/12a_awd_lstm.ipynb\n",
    "> 3. https://github.com/fastai/course-v3/blob/master/nbs/dl2/12b_lm_pretrain.ipynb\n",
    "> 4. https://github.com/fastai/course-v3/blob/master/nbs/dl2/12c_ulmfit.ipynb\n",
    "> \n",
    "> Video:\n",
    "> - https://youtu.be/vnOpEwmtFJ8?t=4687 from 1:18:00 to 2:08:00 (50 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "BPTT_LEN   = 70 # Lengh of the minisequences in the big stream (used In Dataset)\n",
    "BATCH_SIZE = 64 # (used In DataLoader)\n",
    "UNK_TOKEN  = 0  # Number for xxunk token (used in numericalization)\n",
    "PAD_TOKEN  = 1  # Number for xxPad token (used in the nn.Embedding layer of the Model)\n",
    "VOCAB_LEN  = None # This will be updated when we read the vocab.pkl file (used in the nn.Embedding layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch: 1.8.1\n",
      "Fast.ai: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pathlib\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, Sampler, DataLoader\n",
    "\n",
    "import fastai\n",
    "print(\"Pytorch:\", torch.__version__)\n",
    "print(\"Fast.ai:\", fastai.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test  train  unsup  vocab.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls \"../../Datasets/NLP/IMBd_prepro\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg  pos\r\n"
     ]
    }
   ],
   "source": [
    "!ls \"../../Datasets/NLP/IMBd_prepro/train\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = open(\"../../Datasets/NLP/IMBd_prepro/vocab.pkl\",'rb')\n",
    "imdb_vocab = pickle.load(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65539"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_LEN = len(imdb_vocab)\n",
    "VOCAB_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize(word):\n",
    "    try:    return imdb_vocab.index(word)\n",
    "    except: return UNK_TOKEN\n",
    "    \n",
    "def denumericalize(num):\n",
    "    return imdb_vocab[num]\n",
    "\n",
    "def numericalize_list(words):\n",
    "    return [numericalize(word) for word in words]\n",
    "\n",
    "def denumericalize_list(nums):\n",
    "    return [denumericalize(num) for num in nums]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5255, 195, 0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numericalize_list( [\"hello\", \"world\", \"this_is_unknown_token\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxunk',\n",
       " 'xxpad',\n",
       " 'xxbos',\n",
       " 'xxeos',\n",
       " 'xxrep',\n",
       " 'xxwrep',\n",
       " 'xxup',\n",
       " 'xxmaj',\n",
       " 'the',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denumericalize_list( range(10) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "def parallel_map(func, array):\n",
    "    \n",
    "    cpu_cores = multiprocessing.cpu_count()\n",
    "    array_len = len(array)\n",
    "    chunksize = array_len // 100\n",
    "    \n",
    "    if cpu_cores<2:\n",
    "        return list(tqdm(map(func, arr), total=array_len))\n",
    "    else:\n",
    "        with ProcessPoolExecutor(max_workers=cpu_cores) as ex:\n",
    "            return list(tqdm(ex.map(func, array, chunksize=chunksize), total=array_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <center> Part 1: Dataset & Dataloader for Langauge Model\n",
    "- X: Text\n",
    "- Y: Same text but shifted by 1 token\n",
    "\n",
    "At every epoch:\n",
    "\n",
    "1. **Shuffle** (sort randomly) our collection of texts.\n",
    "2. **Concatenate** the individual texts together into a big stream. \n",
    "3. **Cut** this stream into a certain number of batches (which is our batch size).\n",
    "   - For instance, if the stream has 50,000 tokens and we set a batch size of 10, this will give us 10 mini-streams of 5,000 tokens.\n",
    "   \n",
    "So to recap, at every epoch we shuffle our collection of documents and concatenate them into a stream of tokens. We then cut that stream into a batch of fixed-size consecutive mini-streams. Our model will then read the mini-streams in order, and thanks to an inner state, it will produce the same activation whatever sequence length we picked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, prepro_texts_dir, bptt=70, shuffle=False):\n",
    "        \n",
    "        # Read tokenized and numeralized text files (numpy format)\n",
    "        np_filepaths = list( pathlib.Path(prepro_texts_dir).glob('**/*.npy') ) \n",
    "        \n",
    "        # Open numpy arrays in parallel\n",
    "        #self.texts_np = list(map(func=np.load, np_filepaths))  # Non parallel version   \n",
    "        self.texts_np = parallel_map(func=np.load, array=np_filepaths)\n",
    "\n",
    "        self.bptt    = bptt\n",
    "        self.shuffle = shuffle\n",
    "        self.total_tokens = sum([len(t) for t in self.texts_np])\n",
    "        \n",
    "        self.concat_texts_into_stream()\n",
    "        \n",
    "    # this is necesseary at the begining of every epoch for train !!!!\n",
    "    def concat_texts_into_stream(self):\n",
    "        \n",
    "        # 1. Reorder texts if we need to\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.texts_np)\n",
    "            #self.texts_np = self.texts_np[np.random.permutation(len(self.texts_np))]\n",
    "            \n",
    "        # 2. Concat texts into a large stream\n",
    "        self.stream = np.concatenate(self.texts_np)\n",
    "        #self.stream = torch.cat([torch.Tensor(t) for t in self.texts_np])\n",
    "                \n",
    "    def __len__(self):\n",
    "        return self.total_tokens // self.bptt\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.stream[idx   : idx+self.bptt]\n",
    "        y = self.stream[idx+1 : idx+self.bptt+1] # shifted by 1\n",
    "        \n",
    "        # convert from numpy.uint16 to torch.int64        \n",
    "        x = torch.tensor(x.astype(\"int64\"))\n",
    "        y = torch.tensor(y.astype(\"int64\"))\n",
    "        \n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69454872af8c472cb9869e74eebaa43d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c41e6cf08a294b2e97bb4f2064d3de90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_ds = LM_Dataset(\"../../Datasets/NLP/IMBd_prepro/train\", bptt=BPTT_LEN, shuffle=True)\n",
    "valid_ds = LM_Dataset(\"../../Datasets/NLP/IMBd_prepro/test\",  bptt=BPTT_LEN, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([    7,   748,   174,   119,     8,   231,     7, 17098,    10,   175,\n",
       "          4762,   201,   702,    15,  8724,    47,     7,   738,  5242,     7,\n",
       "          4156,    11,     7,  1885,     7,   732,    95,     7, 17098,   263,\n",
       "             9,     7,    19,    75,    54,    38,    81,   297,   573,   320,\n",
       "            13,   748,     9,     7,   622,    13,     8,   297,   573,    15,\n",
       "            83,   231,    55,  8338,   371,     9,     7,     8,   106,    82,\n",
       "            15,    12,   201,  7852,    10,  4326,    82,    59,    12,  1673]),\n",
       " tensor([  748,   174,   119,     8,   231,     7, 17098,    10,   175,  4762,\n",
       "           201,   702,    15,  8724,    47,     7,   738,  5242,     7,  4156,\n",
       "            11,     7,  1885,     7,   732,    95,     7, 17098,   263,     9,\n",
       "             7,    19,    75,    54,    38,    81,   297,   573,   320,    13,\n",
       "           748,     9,     7,   622,    13,     8,   297,   573,    15,    83,\n",
       "           231,    55,  8338,   371,     9,     7,     8,   106,    82,    15,\n",
       "            12,   201,  7852,    10,  4326,    82,    59,    12,  1673,     7]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([70])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('xxbos xxmaj five years after the original xxmaj creepshow , another inferior horror sequel is penned by xxmaj george a. xxmaj romero and xxmaj stephen xxmaj king : xxmaj creepshow 2 . xxmaj this time there are only three stories instead of five . xxmaj none of the three stories is really original or distinguished either . xxmaj the first story is a horror staple , formulaic story about a',\n",
       " 'xxmaj five years after the original xxmaj creepshow , another inferior horror sequel is penned by xxmaj george a. xxmaj romero and xxmaj stephen xxmaj king : xxmaj creepshow 2 . xxmaj this time there are only three stories instead of five . xxmaj none of the three stories is really original or distinguished either . xxmaj the first story is a horror staple , formulaic story about a wooden')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(denumericalize_list(train_ds[0][0])), \" \".join(denumericalize_list(train_ds[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Custom sampler for BPTT (Backpropagation through time)\n",
    "\n",
    "if we divide our big stream of **28 elements** with **batch_size of 5**:\n",
    "\n",
    "|               |              |               |               |               |               |\n",
    "|---------------|--------------|---------------|---------------|---------------|---------------|\n",
    "| **1st batch** | stream_idx 0 | stream_idx 6  | stream_idx 12 | stream_idx 18 | stream_idx 23 |\n",
    "| **2nd batch** | stream_idx 1 | stream_idx 7  | stream_idx 13 | stream_idx 19 | stream_idx 24 |\n",
    "| **3rd batch** | stream_idx 2 | stream_idx 8  | stream_idx 14 | stream_idx 20 | stream_idx 25 |\n",
    "| **4th batch** | stream_idx 3 | stream_idx 8  | stream_idx 15 | stream_idx 21 | stream_idx 26 |\n",
    "| **5th batch** | stream_idx 4 | stream_idx 10 | stream_idx 16 | stream_idx 22 | stream_idx 27 |\n",
    "| **6th batch** | stream_idx 5 | stream_idx 11 | stream_idx 17 |               |               |\n",
    "\n",
    "https://pytorchnlp.readthedocs.io/en/latest/_modules/torchnlp/samplers/bptt_sampler.html\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPTT_BatchSampler(Sampler):\n",
    "    def __init__(self, n_elements, batch_size, drop_last):\n",
    "        \n",
    "        indexes = np.array_split(list(range(n_elements)), batch_size) # magic happens here\n",
    "        \n",
    "        n_batches = n_elements//batch_size\n",
    "        self.batches_idxs = np.array([x[:n_batches] for x in indexes]).T.tolist()\n",
    "        \n",
    "        if not drop_last:\n",
    "            last_batch_idxs = np.array([x[n_batches:] for x in indexes if x[n_batches:].size==1]).T.tolist()\n",
    "            self.batches_idxs += last_batch_idxs\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return iter(self.batches_idxs)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.batches_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,\n",
       " [[0, 6, 12, 18, 23],\n",
       "  [1, 7, 13, 19, 24],\n",
       "  [2, 8, 14, 20, 25],\n",
       "  [3, 9, 15, 21, 26],\n",
       "  [4, 10, 16, 22, 27],\n",
       "  [5, 11, 17]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = BPTT_BatchSampler(n_elements=28, batch_size=5, drop_last=False)\n",
    "len(s), list(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " [[0, 6, 12, 18, 23],\n",
       "  [1, 7, 13, 19, 24],\n",
       "  [2, 8, 14, 20, 25],\n",
       "  [3, 9, 15, 21, 26],\n",
       "  [4, 10, 16, 22, 27]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = BPTT_BatchSampler(n_elements=28, batch_size=5, drop_last=True)\n",
    "len(s), list(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Dataloader (with custom sampler for BPTT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 70]), torch.Size([64, 70]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dl = DataLoader(train_ds, batch_sampler=BPTT_BatchSampler(n_elements=len(train_ds),\n",
    "                                                                batch_size=BATCH_SIZE,\n",
    "                                                                drop_last=True))\n",
    "\n",
    "valid_dl = DataLoader(valid_ds, batch_sampler=BPTT_BatchSampler(n_elements=len(valid_ds),\n",
    "                                                                batch_size=BATCH_SIZE,\n",
    "                                                                drop_last=True))\n",
    "\n",
    "batch_x, batch_y = next(iter(train_dl))\n",
    "batch_x.shape, batch_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Part 2: Pretrained (on wikipedia) LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Get model layer structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): AWD_LSTM(\n",
       "    (encoder): Embedding(60000, 400, padding_idx=1)\n",
       "    (encoder_dp): EmbeddingDropout(\n",
       "      (emb): Embedding(60000, 400, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDropout(\n",
       "        (module): LSTM(400, 1152, batch_first=True)\n",
       "      )\n",
       "      (1): WeightDropout(\n",
       "        (module): LSTM(1152, 1152, batch_first=True)\n",
       "      )\n",
       "      (2): WeightDropout(\n",
       "        (module): LSTM(1152, 400, batch_first=True)\n",
       "      )\n",
       "    )\n",
       "    (input_dp): RNNDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): RNNDropout()\n",
       "      (1): RNNDropout()\n",
       "      (2): RNNDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=60000, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_language_model(arch=AWD_LSTM, vocab_sz=60000 )\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Put wikipedia pretrained weights into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/home/javi/.fastai/data/wt103-fwd')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = untar_data(URLs.WT103_FWD)\n",
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_vocab_filepath   = model_path / \"itos_wt103.pkl\"\n",
    "wikipedia_weights_filepath = model_path / \"lstm_fwd.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open_file(wikipedia_vocab_filepath, 'rb') as file:\n",
    "    wikipedia_vocab = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wikipedia_weights = torch.load(wikipedia_weights_filepath, map_location = lambda storage,loc: storage)\n",
    "wikipedia_weights = clean_raw_keys(wikipedia_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(wikipedia_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Freeze all model (requires_grad = False for every layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Put new words (of IMDb) in the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki vocab: 60000\n",
      "imdb vocab: 65539\n",
      "---------------------\n",
      "wiki only vocab: 22951\n",
      "imdb only vocab: 28490 (43.47% of imdb)\n",
      "in common vocab: 37049 (56.53% of imdb)\n"
     ]
    }
   ],
   "source": [
    "wiki_only_vocab = len(set(wikipedia_vocab) - set(imdb_vocab))\n",
    "imdb_only_vocab = len(set(imdb_vocab) - set(wikipedia_vocab))\n",
    "in_common_vocab = len(set(imdb_vocab) & set(wikipedia_vocab))\n",
    "\n",
    "print(\"wiki vocab:\", len(wikipedia_vocab))\n",
    "print(\"imdb vocab:\", len(imdb_vocab))\n",
    "print(\"---------------------\")\n",
    "print(f\"wiki only vocab: {wiki_only_vocab}\")\n",
    "print(f\"imdb only vocab: {imdb_only_vocab} ({round(imdb_only_vocab/len(imdb_vocab)*100,2)}% of imdb)\")\n",
    "print(f\"in common vocab: {in_common_vocab} ({round(in_common_vocab/len(imdb_vocab)*100,2)}% of imdb)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 400])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1276,  0.0161,  0.1617,  ..., -0.1396,  0.6899, -0.0399],\n",
       "        [ 0.0272,  0.0011,  0.0401,  ...,  0.0161,  0.0666, -0.0014],\n",
       "        [ 0.6069, -0.5239,  0.1544,  ..., -0.2551, -0.3308, -0.0702],\n",
       "        ...,\n",
       "        [ 0.0096,  0.0814,  0.0213,  ...,  0.0712,  0.0810, -0.0045],\n",
       "        [ 0.0283, -0.0176,  0.0361,  ...,  0.0756,  0.1470, -0.0139],\n",
       "        [ 0.1267, -0.0656,  0.0362,  ..., -0.0191,  0.0673,  0.0154]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_emb_matrix = model[0].encoder.weight.data\n",
    "print(old_emb_matrix.shape)\n",
    "old_emb_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3336e5ce3a5c482e9ee837a5d32f817f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/65539 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_emb_matrix = torch.zeros( (len(imdb_vocab), old_emb_matrix.size(1)) )\n",
    "old_emb_matrix_mean = old_emb_matrix.mean(0)\n",
    "\n",
    "for i,word in tqdm(enumerate(imdb_vocab), total=len(imdb_vocab)):\n",
    "    \n",
    "    try:\n",
    "        # If the word exits on the wikipedia vocab -> Use existing embedding\n",
    "        old_emb_idx = wikipedia_vocab.index(word)\n",
    "        new_emb_matrix[i] = old_emb_matrix[old_emb_idx]\n",
    "    except:\n",
    "        # If the word is new -> Use default embedding (the mean)\n",
    "        new_emb_matrix[i] = old_emb_matrix_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65539, 400])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1276,  0.0161,  0.1617,  ..., -0.1396,  0.6899, -0.0399],\n",
       "        [ 0.0272,  0.0011,  0.0401,  ...,  0.0161,  0.0666, -0.0014],\n",
       "        [ 0.6069, -0.5239,  0.1544,  ..., -0.2551, -0.3308, -0.0702],\n",
       "        ...,\n",
       "        [-0.1464,  0.0314,  0.1414,  ..., -0.1256,  0.2263, -0.2686],\n",
       "        [ 0.0253,  0.0026,  0.0423,  ...,  0.0128,  0.0848, -0.0018],\n",
       "        [ 0.0253,  0.0026,  0.0423,  ...,  0.0128,  0.0848, -0.0018]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(new_emb_matrix.shape)\n",
    "new_emb_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model[0].encoder = nn.Embedding(num_embeddings = new_emb_matrix.shape[0],\n",
    "                                embedding_dim  = new_emb_matrix.shape[1],\n",
    "                                padding_idx    = 1,\n",
    "                                _weight        = new_emb_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model[0].encoder_dp.emb = nn.Embedding(num_embeddings = new_emb_matrix.shape[0],\n",
    "                                embedding_dim  = new_emb_matrix.shape[1],\n",
    "                                padding_idx    = 1,\n",
    "                                _weight        = new_emb_matrix)\n",
    "\n",
    "model[0].encoder_dp.emb.weight = model[0].encoder.weight # Weight tiying !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model[1].decoder = nn.Linear(in_features  = new_emb_matrix.shape[1],\n",
    "                             out_features = new_emb_matrix.shape[0],\n",
    "                             bias = True)\n",
    "\n",
    "model[1].decoder.weight = model[0].encoder.weight # Weight tiying !!\n",
    "model[1].decoder.bias.data.zero_();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for parameter in model.parameters():\n",
    "    print(parameter.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): AWD_LSTM(\n",
       "    (encoder): Embedding(65539, 400, padding_idx=1)\n",
       "    (encoder_dp): EmbeddingDropout(\n",
       "      (emb): Embedding(65539, 400, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDropout(\n",
       "        (module): LSTM(400, 1152, batch_first=True)\n",
       "      )\n",
       "      (1): WeightDropout(\n",
       "        (module): LSTM(1152, 1152, batch_first=True)\n",
       "      )\n",
       "      (2): WeightDropout(\n",
       "        (module): LSTM(1152, 400, batch_first=True)\n",
       "      )\n",
       "    )\n",
       "    (input_dp): RNNDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): RNNDropout()\n",
       "      (1): RNNDropout()\n",
       "      (2): RNNDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=65539, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model\n",
    "We return three things:\n",
    "1. The true output (probabilities for each word),\n",
    "2. The activations of the encoder with dropouts. (to help with regularization)\n",
    "3. The activations of the encoder without dropouts. to help with regularization\n",
    "\n",
    "Becouse of this, we need to use the [RNNCallback](https://docs.fast.ai/callback.rnn.html#RNNCallback) to only keep the true output for loss computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 70, 65539]),\n",
       " torch.Size([64, 70, 400]),\n",
       " torch.Size([64, 70, 400]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_p, hid1, hid1 = model(batch_x) # prediction and something else\n",
    "batch_p.shape, hid1.shape, hid1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Part 3: Loss (Flat CrossEntropy)\n",
    "\n",
    "Custom CrossEntropyLossFlat inspired by [Fast.ai CrossEntropyLossFlat()](https://docs.fast.ai/losses.html#CrossEntropyLossFlat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred:   torch.Size([64, 70, 65539])\n",
      "Target: torch.Size([64, 70])\n"
     ]
    }
   ],
   "source": [
    "print(\"Pred:  \", batch_p.shape)\n",
    "print(\"Target:\", batch_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred:   torch.Size([4480, 65539])\n",
      "Target: torch.Size([4480])\n"
     ]
    }
   ],
   "source": [
    "batch_p_flatten = batch_p.view(-1, batch_p.shape[-1])\n",
    "batch_y_flatten = batch_y.view(-1)\n",
    "\n",
    "print(\"Pred:  \", batch_p_flatten.shape)\n",
    "print(\"Target:\", batch_y_flatten.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLossFlat(nn.Module):\n",
    "    \"\"\"Flatten version of nn.CrossEntropyLoss()\"\"\"\n",
    "    def forward(self, pred, target):\n",
    "        pred_flatten   = pred.view(-1, pred.shape[-1])\n",
    "        target_flatten = target.view(-1)\n",
    "        return F.cross_entropy(pred_flatten, target_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.4443, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "ce_loss(batch_p_flatten, batch_y_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.4443, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ce_flat_loss = CrossEntropyLossFlat()\n",
    "ce_flat_loss(batch_p, batch_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Part 4: Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Fast.ai: 2.3.1\n",
    "from fastai.learner           import Learner\n",
    "from fastai.data.core         import DataLoaders\n",
    "from fastai.metrics           import accuracy, Perplexity\n",
    "\n",
    "from fastai.callback.all      import * # For lr_find() and fit_one_cycle()\n",
    "from fastai.callback.core     import Callback\n",
    "from fastai.callback.progress import ProgressCallback, ShowGraphCallback\n",
    "from fastai.callback.training import GradientClip\n",
    "from fastai.callback.rnn      import ModelResetter, RNNCallback, RNNRegularizer\n",
    "from fastai.callback.tracker  import EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShowGraphEveryBatchCallback(Callback):\n",
    "    \"Update a graph of training and validation loss\"\n",
    "        \n",
    "    def after_batch(self):\n",
    "        if self.iter%15 == 0:\n",
    "            rec   = self.learn.recorder\n",
    "            iters = range(len(rec.losses))\n",
    "            x_bounds = (0, self.n_epoch * self.n_iter)\n",
    "            y_bounds = (0, 10)\n",
    "            self.progress.mbar.update_graph([(iters, rec.losses)], x_bounds, y_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls       = DataLoaders(train_dl, valid_dl),\n",
    "                model     = model, \n",
    "                loss_func = CrossEntropyLossFlat(),\n",
    "                metrics   = [accuracy, Perplexity],\n",
    "                wd        = 0.1,\n",
    "                cbs       = [\n",
    "                          #  TrainEvalCallback, # By default\n",
    "                          #  Recorder,          # By default\n",
    "                          #  ProgressCallback,  # By default\n",
    "                             ModelResetter,\n",
    "                             RNNCallback,\n",
    "                             RNNRegularizer(alpha=2., beta=1.), # Add AR and TAR regularization\n",
    "                             ShowGraphEveryBatchCallback,\n",
    "                             GradientClip(max_norm=0.1),        # Clip norm of gradients\n",
    "                             MixedPrecision,                    # .to_fp16()\n",
    "                          #  TensorBoardCallback(\"/home/javi/Escritorio/tensorboard\", trace_model=True)\n",
    "                          #  partial(AvgStatsCallback,accuracy_flat),\n",
    "                          #  Recorder,  #     Registers statistics (lr, loss and metrics) during training\n",
    "                          #  partial(GradientClipping, clip=0.1),\n",
    "                          #  partial(RNNTrainer, α=2., β=1.),\n",
    "                          #  ParamScheduler({'lr': SchedLin(1e-3, 1e-2)})\n",
    "                          #  EarlyStoppingCallback(patience=3)\n",
    "                          #  SaveModelCallback()\n",
    "                            ])\n",
    "#learn.show_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.lr_find()\n",
    "#fig = learn.recorder.plot(suggestion=True, return_fig=True);\n",
    "#lr  = learn.recorder.min_grad_lr "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Train only embeddings here (the LSTM are frozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.019652</td>\n",
       "      <td>4.980197</td>\n",
       "      <td>0.239088</td>\n",
       "      <td>145.503036</td>\n",
       "      <td>08:37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Experiment      |Epochs| LR        |frozen| wd  | fp16 | reg | train loss | valid loss | accuracy   | perplexity |time | notes                   |\n",
    "|:----------------|:----:|:---------:|:----:|:---:|:----:|:---:|-----------:|-----------:|:----------:|-----------:|:---:|:------------------------|\n",
    "| From scratch a  |  1   |           |   no |  no |  no  |  no | 19.061800  | 45.006886  |  0.090237  |            |09:10|                         |\n",
    "| From scratch b  |  1   |           |   no |  no |  no  |  no |  5.943309  |  7.154285  |  0.090238  |1279.577148 |09:38| only predicts `xxmaj`   |\n",
    "| From scratch c  |  1   |Const 1e-3 |   no |  no |  no  |  no |  1.992864  |  5.868248  |  0.197092  | 353.628693 |09:18|                         |\n",
    "| From scratch d  |  1   | OC 1e-2   |   no | yes | yes  | yes |  5.362375  |  5.571612  |  0.181193  | 262.857574 |09:23| min train loss was 1.   |\n",
    "|**From scratch e**|  1   | OC 1e-2   |  yes | yes | yes  | yes |  5.019652  |**4.980197**|**0.239088**| 145.503036 |08:37| min train loss was 2.   |\n",
    "|**Fastai high level**|  1   | OC 1e-2   |  yes | yes | yes  | yes |  4.390143  |**4.170024**|**0.281579**|  64.717033 |09:59| using high level Fastai |\n",
    "\n",
    "- **reg**: Means `RNNRegularizer(alpha=2., beta=1.)`\n",
    "- **wd**: Means `wd = 0.1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Unfreeze anf train whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "learn.fit_one_cycle(10, 1e-3)\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_p,_,_= learn.model(batch_x.cuda())\n",
    "#batch_p = batch_p.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorBase(0.2406, device='cuda:0')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(batch_p, batch_y.cuda()) # On some train batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    7,     8,   174,  ...,    10,    12,   145],\n",
       "        [    8,     7, 12139,  ...,    15,    12,    42],\n",
       "        [    7,     2,     7,  ...,    13,    39,    11],\n",
       "        ...,\n",
       "        [    7,    59,     7,  ...,    27,   204,     9],\n",
       "        [   38,    46,    86,  ...,     7,     8,    25],\n",
       "        [   12,    11,     8,  ...,     8,    10,    71]], device='cuda:0')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = batch_p.argmax(dim=-1)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxmaj',\n",
       " 'the',\n",
       " 'years',\n",
       " 'after',\n",
       " 'the',\n",
       " 'xxmaj',\n",
       " 'xxmaj',\n",
       " 'french',\n",
       " '2',\n",
       " 'xxmaj',\n",
       " 'xxmaj',\n",
       " 'xxmaj',\n",
       " 'movie',\n",
       " ',',\n",
       " 'made',\n",
       " 'by',\n",
       " 'xxmaj',\n",
       " 'paul',\n",
       " 'xxmaj',\n",
       " 'xxmaj',\n",
       " 'documentary',\n",
       " ',',\n",
       " 'xxmaj',\n",
       " 'jean',\n",
       " 'xxmaj',\n",
       " 'king',\n",
       " ',',\n",
       " 'xxmaj',\n",
       " 'the',\n",
       " '2',\n",
       " ':',\n",
       " 'xxmaj',\n",
       " 'this',\n",
       " 'movie',\n",
       " ',',\n",
       " 'was',\n",
       " 'many',\n",
       " '3',\n",
       " 'films',\n",
       " 'of',\n",
       " 'of',\n",
       " 'the',\n",
       " 'films',\n",
       " 'xxmaj',\n",
       " 'this',\n",
       " 'of',\n",
       " 'them',\n",
       " 'films',\n",
       " 'films',\n",
       " 'of',\n",
       " 'a',\n",
       " 'bad',\n",
       " '.',\n",
       " 'original',\n",
       " '.',\n",
       " '.',\n",
       " 'xxmaj',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'one',\n",
       " 'is',\n",
       " 'a',\n",
       " 'movie',\n",
       " 'movie',\n",
       " ',',\n",
       " 'and',\n",
       " 'and',\n",
       " ',',\n",
       " 'a',\n",
       " 'man',\n",
       " 'the',\n",
       " 'xxmaj',\n",
       " 'calibre',\n",
       " ',',\n",
       " 'xxmaj',\n",
       " 'xxmaj',\n",
       " 'xxmaj',\n",
       " 'of',\n",
       " 'and',\n",
       " 'cinematography',\n",
       " 'state',\n",
       " ',',\n",
       " 'the',\n",
       " ',',\n",
       " 'the',\n",
       " 'few',\n",
       " 'cast',\n",
       " 'stardom',\n",
       " '.',\n",
       " 'xxmaj',\n",
       " 'paul',\n",
       " 'xxmaj',\n",
       " 'xxunk',\n",
       " '.',\n",
       " 'xxmaj',\n",
       " 'the',\n",
       " 'xxmaj',\n",
       " 'acted',\n",
       " 'is',\n",
       " 'a',\n",
       " 'good',\n",
       " 'in',\n",
       " 'a',\n",
       " 'writer',\n",
       " 'writer',\n",
       " 'believable',\n",
       " 'writer',\n",
       " ',',\n",
       " 'does',\n",
       " 'is',\n",
       " 'a',\n",
       " 'ann',\n",
       " 'xxmaj',\n",
       " 'arquette',\n",
       " 'who',\n",
       " 'is',\n",
       " 'the',\n",
       " 'movie',\n",
       " '.',\n",
       " 'a',\n",
       " 'xxunk',\n",
       " 'xxmaj',\n",
       " 'seems',\n",
       " 'is',\n",
       " 'xxmaj',\n",
       " 'the',\n",
       " 'is',\n",
       " 'a',\n",
       " 'beautiful',\n",
       " 'who',\n",
       " 'who',\n",
       " ',',\n",
       " 'a',\n",
       " 'who',\n",
       " 'funny',\n",
       " '.',\n",
       " '.',\n",
       " 'is',\n",
       " 'a',\n",
       " 'be',\n",
       " 'xxmaj',\n",
       " 'xxbos',\n",
       " 'xxmaj',\n",
       " 'xxmaj',\n",
       " 'old',\n",
       " 'the',\n",
       " 'her',\n",
       " 'xxmaj',\n",
       " 'american',\n",
       " ',',\n",
       " 'eyre',\n",
       " 'and',\n",
       " 'xxmaj',\n",
       " 'xxunk',\n",
       " 'xxmaj',\n",
       " ')',\n",
       " 'and',\n",
       " 'xxmaj',\n",
       " 'xxmaj',\n",
       " 'xxmaj',\n",
       " 'mr.',\n",
       " 'xxmaj',\n",
       " 'xxmaj',\n",
       " 'xxmaj',\n",
       " 'joseph',\n",
       " 'xxmaj',\n",
       " 'watson',\n",
       " 'and',\n",
       " 'xxmaj',\n",
       " 'xxunk',\n",
       " 'xxmaj',\n",
       " 'simon',\n",
       " ')',\n",
       " 'and',\n",
       " ',',\n",
       " 'xxmaj',\n",
       " 'xxmaj',\n",
       " 'and',\n",
       " 'xxmaj',\n",
       " 'the',\n",
       " 'xxmaj',\n",
       " 'with',\n",
       " 'the',\n",
       " 'the',\n",
       " 'xxmaj',\n",
       " 'xxmaj',\n",
       " 'of',\n",
       " 'hands',\n",
       " ',',\n",
       " 'xxmaj',\n",
       " 'the',\n",
       " 'his',\n",
       " 'with',\n",
       " 'the',\n",
       " ',',\n",
       " 'he',\n",
       " 'gets',\n",
       " 'gets',\n",
       " 'a',\n",
       " 'take',\n",
       " 'a',\n",
       " 'from',\n",
       " 'kind',\n",
       " 'from',\n",
       " 'xxmaj',\n",
       " 'xxmaj',\n",
       " 'xxmaj',\n",
       " 'of',\n",
       " 'he',\n",
       " 'and',\n",
       " 'xxmaj',\n",
       " 'the',\n",
       " 'xxmaj',\n",
       " 'huston',\n",
       " ',',\n",
       " 'his',\n",
       " 'role',\n",
       " ',',\n",
       " 'because',\n",
       " 'the',\n",
       " 'the',\n",
       " 'xxmaj',\n",
       " 'he',\n",
       " 'xxmaj',\n",
       " 'rambo',\n",
       " ',',\n",
       " 'xxmaj',\n",
       " 'young',\n",
       " '-',\n",
       " 'headed',\n",
       " 'man',\n",
       " 'who',\n",
       " 'who',\n",
       " 'john',\n",
       " 'xxmaj',\n",
       " 'jones',\n",
       " ',',\n",
       " 'xxmaj',\n",
       " '-',\n",
       " 'up',\n",
       " '.',\n",
       " 'xxmaj',\n",
       " 'to',\n",
       " '.',\n",
       " 'his',\n",
       " 'john',\n",
       " 'xxmaj',\n",
       " 'grodin',\n",
       " 'is',\n",
       " 'his',\n",
       " '.',\n",
       " '.',\n",
       " 'xxmaj',\n",
       " 'the',\n",
       " 'xxmaj',\n",
       " '-',\n",
       " 'of',\n",
       " 'is',\n",
       " 'a',\n",
       " ',',\n",
       " 'the',\n",
       " 'xxmaj',\n",
       " 'the',\n",
       " 'xxmaj',\n",
       " 'years',\n",
       " 'xxmaj',\n",
       " 'women',\n",
       " '\"',\n",
       " 'and',\n",
       " '\"',\n",
       " '\"',\n",
       " '\"',\n",
       " 'movie',\n",
       " 'in',\n",
       " 'up',\n",
       " ',',\n",
       " 'xxmaj',\n",
       " 'the',\n",
       " ',',\n",
       " 'a',\n",
       " 'a',\n",
       " 'have',\n",
       " 'the',\n",
       " '.',\n",
       " 'i',\n",
       " 'is',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'of',\n",
       " 'the',\n",
       " 'worst',\n",
       " 'films',\n",
       " 'i',\n",
       " 'have',\n",
       " 'seen',\n",
       " 'seen',\n",
       " '.',\n",
       " 'i',\n",
       " 'a',\n",
       " 'not',\n",
       " 'best',\n",
       " '.',\n",
       " 'i',\n",
       " 'have',\n",
       " 'seen',\n",
       " '.',\n",
       " 'the',\n",
       " 'time',\n",
       " '.',\n",
       " 'and',\n",
       " 'attempt',\n",
       " 'and',\n",
       " 'from',\n",
       " 'the',\n",
       " 'i',\n",
       " 'have',\n",
       " 'watching',\n",
       " 'it',\n",
       " 'the',\n",
       " ',',\n",
       " 'xxmaj',\n",
       " 'xxmaj',\n",
       " 'screen',\n",
       " ')',\n",
       " '.',\n",
       " 'xxmaj',\n",
       " 'the',\n",
       " 'film',\n",
       " 'films',\n",
       " 'were',\n",
       " 'xxmaj',\n",
       " 'ann',\n",
       " 'xxmaj',\n",
       " 'century',\n",
       " 'and',\n",
       " 'xxmaj',\n",
       " 'paul',\n",
       " 'xxmaj',\n",
       " 'carlyle',\n",
       " ')',\n",
       " 'were',\n",
       " 'the',\n",
       " 'xxunk',\n",
       " 'the',\n",
       " '.',\n",
       " 'were',\n",
       " 'enjoyed',\n",
       " '.',\n",
       " 'the',\n",
       " 'xxunk',\n",
       " 'xxmaj',\n",
       " 'xxunk',\n",
       " ',',\n",
       " 'xxmaj',\n",
       " 'a',\n",
       " 'by',\n",
       " 'xxmaj',\n",
       " 'paul',\n",
       " 'xxmaj',\n",
       " ',',\n",
       " 'the',\n",
       " 'xxmaj',\n",
       " \"'s\",\n",
       " 'xxmaj',\n",
       " '.',\n",
       " 'the',\n",
       " 'and',\n",
       " 'the',\n",
       " 'few',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'into',\n",
       " 'xxunk',\n",
       " 'xxmaj',\n",
       " 'mummy',\n",
       " 'in',\n",
       " 'the',\n",
       " 'the',\n",
       " '.',\n",
       " 'body',\n",
       " 'and',\n",
       " 'xxmaj',\n",
       " 'a',\n",
       " ',',\n",
       " ',',\n",
       " 'the',\n",
       " 'character',\n",
       " 'life',\n",
       " '.',\n",
       " 'the',\n",
       " 'the',\n",
       " 'xxmaj',\n",
       " 'mummy',\n",
       " 'xxmaj',\n",
       " 'xxmaj',\n",
       " 'while',\n",
       " 'xxmaj',\n",
       " 'old',\n",
       " '.',\n",
       " 'xxmaj',\n",
       " 'xxmaj',\n",
       " 'is',\n",
       " 'the',\n",
       " 'xxmaj',\n",
       " 'mummy',\n",
       " 'is',\n",
       " 'a',\n",
       " 'in',\n",
       " 'be',\n",
       " 'a',\n",
       " 'on',\n",
       " 'a',\n",
       " 'a',\n",
       " 'xxmaj',\n",
       " '.',\n",
       " 'cast',\n",
       " 'act',\n",
       " 'xxmaj',\n",
       " ',',\n",
       " 'xxunk',\n",
       " 'xxmaj',\n",
       " 'xxmaj',\n",
       " 'doug',\n",
       " 'xxmaj',\n",
       " \"n't\",\n",
       " 'xxmaj',\n",
       " 'to',\n",
       " 'their',\n",
       " 'in',\n",
       " 'with',\n",
       " 'xxmaj',\n",
       " 'the',\n",
       " 'xxmaj',\n",
       " 'film',\n",
       " 'is',\n",
       " 'up',\n",
       " 'with',\n",
       " 'a',\n",
       " 'a',\n",
       " 'one',\n",
       " 'xxmaj',\n",
       " 'lot',\n",
       " 'in',\n",
       " 'a',\n",
       " 'in',\n",
       " 'in',\n",
       " 'the',\n",
       " 'a',\n",
       " '.',\n",
       " 'xxmaj',\n",
       " 'the',\n",
       " 'would',\n",
       " 'the',\n",
       " 'the',\n",
       " 'joe',\n",
       " 'xxmaj',\n",
       " '?',\n",
       " '?',\n",
       " 'a',\n",
       " 'up',\n",
       " 'with',\n",
       " 'the',\n",
       " 'xxmaj',\n",
       " '.',\n",
       " 'xxmaj',\n",
       " 'the',\n",
       " 'know',\n",
       " 'never',\n",
       " 'say',\n",
       " 'attention',\n",
       " 'hand',\n",
       " 'attention',\n",
       " 'to',\n",
       " 'the',\n",
       " 'joe',\n",
       " 'xxmaj',\n",
       " 'xxmaj',\n",
       " 'are',\n",
       " 'the',\n",
       " 'xxmaj',\n",
       " 'flimsy',\n",
       " 'sadness',\n",
       " 'upon',\n",
       " 'for',\n",
       " '.',\n",
       " 'xxmaj',\n",
       " 'have',\n",
       " 'you',\n",
       " 'xxunk',\n",
       " 'xxmaj',\n",
       " 'door',\n",
       " ',',\n",
       " 'out',\n",
       " 'it',\n",
       " 'the',\n",
       " 'mr.',\n",
       " 'xxmaj',\n",
       " 'schneider',\n",
       " \"'\",\n",
       " 'jr',\n",
       " '.',\n",
       " 'xxmaj',\n",
       " 'xxmaj',\n",
       " ':',\n",
       " 'xxmaj',\n",
       " ')',\n",
       " ')',\n",
       " 'of',\n",
       " 'xxmaj',\n",
       " 'footage',\n",
       " 'of',\n",
       " 'xxmaj',\n",
       " 'the',\n",
       " 'is',\n",
       " 'a',\n",
       " 'great',\n",
       " 'actor',\n",
       " 'talented',\n",
       " '-',\n",
       " '.',\n",
       " 'and',\n",
       " 'a',\n",
       " 'be',\n",
       " 'a',\n",
       " 'no',\n",
       " 'performances',\n",
       " 'xxmaj',\n",
       " 'she',\n",
       " 'is',\n",
       " 'her',\n",
       " 'to',\n",
       " 'the',\n",
       " 'character',\n",
       " '.',\n",
       " 'she',\n",
       " 'it',\n",
       " 'film',\n",
       " 'a',\n",
       " '.',\n",
       " 'xxmaj',\n",
       " 'she',\n",
       " 'a',\n",
       " 'being',\n",
       " 'she',\n",
       " \"'s\",\n",
       " 'be',\n",
       " 'been',\n",
       " 'to',\n",
       " 'xxmaj',\n",
       " 'xxmaj',\n",
       " 'the',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'good',\n",
       " 'and',\n",
       " 'xxmaj',\n",
       " 'xxmaj',\n",
       " '\"',\n",
       " 'a',\n",
       " 'hilarious',\n",
       " 'with',\n",
       " 'xxmaj',\n",
       " 'xxmaj',\n",
       " 'the',\n",
       " 'was',\n",
       " 'many',\n",
       " 'to',\n",
       " 'many',\n",
       " 'of',\n",
       " 'for',\n",
       " 'could',\n",
       " 'stephen',\n",
       " 'was',\n",
       " 'to',\n",
       " 'make',\n",
       " '.',\n",
       " 'make',\n",
       " 'of',\n",
       " 'them',\n",
       " '.',\n",
       " '.',\n",
       " 'xxmaj',\n",
       " 'the',\n",
       " 'the',\n",
       " 'xxmaj',\n",
       " ')',\n",
       " 'type',\n",
       " ',',\n",
       " '.',\n",
       " ',',\n",
       " ')',\n",
       " '.',\n",
       " 'much',\n",
       " '.',\n",
       " 'the',\n",
       " '.',\n",
       " 'xxmaj',\n",
       " 'the',\n",
       " 'is',\n",
       " 'many',\n",
       " 'very',\n",
       " 'and',\n",
       " 'american',\n",
       " 'and',\n",
       " 'but',\n",
       " 'course',\n",
       " 'xxmaj',\n",
       " 'that',\n",
       " 'are',\n",
       " 'up',\n",
       " 'a',\n",
       " 'few',\n",
       " 'in',\n",
       " 'the',\n",
       " 'xxmaj',\n",
       " '.',\n",
       " '.',\n",
       " 'xxmaj',\n",
       " 'have',\n",
       " \"n't\",\n",
       " 'know',\n",
       " 'know',\n",
       " 'the',\n",
       " 'the',\n",
       " 'was',\n",
       " 'place',\n",
       " 'in',\n",
       " 'xxmaj',\n",
       " ',',\n",
       " 'blood',\n",
       " \"n't\",\n",
       " 'be',\n",
       " 'to',\n",
       " 'of',\n",
       " 'the',\n",
       " '.',\n",
       " 'xxmaj',\n",
       " 'are',\n",
       " 'xxmaj',\n",
       " 'book',\n",
       " 'man',\n",
       " ',',\n",
       " '.',\n",
       " 'blood',\n",
       " '.',\n",
       " 'xxmaj',\n",
       " '.',\n",
       " 'xxmaj',\n",
       " 'xxmaj',\n",
       " 'the',\n",
       " 'xxmaj',\n",
       " 'dixon',\n",
       " 'is',\n",
       " 'you',\n",
       " 'film',\n",
       " 'sequences',\n",
       " 'a',\n",
       " 'on',\n",
       " 'the',\n",
       " 'film',\n",
       " '.',\n",
       " 'and',\n",
       " 'it',\n",
       " 'is',\n",
       " 'to',\n",
       " 'convince',\n",
       " 'missed',\n",
       " '.',\n",
       " 'it',\n",
       " 'film',\n",
       " 'is',\n",
       " 'very',\n",
       " 'slow',\n",
       " 'acted',\n",
       " '.',\n",
       " 'xxmaj',\n",
       " 'the',\n",
       " 'is',\n",
       " 'a',\n",
       " 'that',\n",
       " 'see',\n",
       " 'the',\n",
       " 'the',\n",
       " 'the',\n",
       " 'is',\n",
       " 'happening',\n",
       " 'on',\n",
       " '.',\n",
       " 'but',\n",
       " 'it',\n",
       " 'the',\n",
       " 'it',\n",
       " 'is',\n",
       " 'a',\n",
       " 'much',\n",
       " 'things',\n",
       " 'things',\n",
       " 'that',\n",
       " 'a',\n",
       " 'the',\n",
       " 'to',\n",
       " 'xxmaj',\n",
       " 'the',\n",
       " 'xxmaj',\n",
       " 'who',\n",
       " 'watched',\n",
       " 'this',\n",
       " 'the',\n",
       " 'xxmaj',\n",
       " 'in',\n",
       " 'to',\n",
       " 'the',\n",
       " 'was',\n",
       " 'out',\n",
       " 'the',\n",
       " '.',\n",
       " 'but',\n",
       " 'the',\n",
       " 'that',\n",
       " 'was',\n",
       " 'be',\n",
       " 'up',\n",
       " '.',\n",
       " 'the',\n",
       " 'was',\n",
       " '.',\n",
       " 'xxmaj',\n",
       " 'the',\n",
       " 'problem',\n",
       " 'who',\n",
       " 'got',\n",
       " 'off',\n",
       " 'during',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'of',\n",
       " 'of',\n",
       " 'xxmaj',\n",
       " 'was',\n",
       " 'myself',\n",
       " 'laughing',\n",
       " '-',\n",
       " '.',\n",
       " '.',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'july',\n",
       " '.',\n",
       " 'of',\n",
       " 'i',\n",
       " 'been',\n",
       " 'to',\n",
       " 'say',\n",
       " 'with',\n",
       " 'the',\n",
       " 'movie',\n",
       " '.',\n",
       " 'xxmaj',\n",
       " 'was',\n",
       " 'the',\n",
       " 'so',\n",
       " 'the',\n",
       " 'first',\n",
       " 'of',\n",
       " '.',\n",
       " 'the',\n",
       " 'was',\n",
       " 'with',\n",
       " 'the',\n",
       " 'and',\n",
       " 'the',\n",
       " 'movie',\n",
       " ',',\n",
       " 'xxmaj',\n",
       " 'was',\n",
       " 'a',\n",
       " 'way',\n",
       " 'to',\n",
       " 'make',\n",
       " 'both',\n",
       " 'cash',\n",
       " 'french',\n",
       " 'of',\n",
       " 'xxmaj',\n",
       " 'jones',\n",
       " 'into',\n",
       " 'into',\n",
       " 'the',\n",
       " 'country',\n",
       " '.',\n",
       " 'but',\n",
       " 'one',\n",
       " 'of',\n",
       " 'and',\n",
       " 'way',\n",
       " ',',\n",
       " 'xxmaj',\n",
       " 'the',\n",
       " ',',\n",
       " 'the',\n",
       " \"'m\",\n",
       " 'the',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'entertainment',\n",
       " 'films',\n",
       " ',',\n",
       " 'well',\n",
       " 'been',\n",
       " 'out',\n",
       " 'out',\n",
       " '-',\n",
       " \"'\",\n",
       " '.',\n",
       " 'xxmaj',\n",
       " 'the',\n",
       " 'of',\n",
       " 'house',\n",
       " 'history',\n",
       " 'xxunk',\n",
       " '.',\n",
       " 'have',\n",
       " 'often',\n",
       " 'have',\n",
       " 'been',\n",
       " 'out',\n",
       " 'big',\n",
       " 'years',\n",
       " 'ago',\n",
       " '.',\n",
       " 'xxmaj',\n",
       " 'xxmaj',\n",
       " 'the',\n",
       " 'the',\n",
       " 'the',\n",
       " 'the',\n",
       " 'film',\n",
       " 'of',\n",
       " 'xxmaj',\n",
       " 'is',\n",
       " 'have',\n",
       " 'as',\n",
       " 'it',\n",
       " 'should',\n",
       " \"n't\",\n",
       " 'see',\n",
       " 'it',\n",
       " 'it',\n",
       " 'was',\n",
       " 'have',\n",
       " 'called',\n",
       " 'bad',\n",
       " 'as',\n",
       " 'the',\n",
       " 'was',\n",
       " 'was',\n",
       " '.',\n",
       " 'xxmaj',\n",
       " 'is',\n",
       " 'a',\n",
       " 'only',\n",
       " 'of',\n",
       " 'the',\n",
       " '.',\n",
       " 'xxmaj',\n",
       " 'have',\n",
       " ',',\n",
       " 'is',\n",
       " 'a',\n",
       " ',',\n",
       " 'xxmaj',\n",
       " 'have',\n",
       " 'that',\n",
       " 'is',\n",
       " 'be',\n",
       " 'a',\n",
       " '.',\n",
       " 'but',\n",
       " 'it',\n",
       " 'was',\n",
       " 'be',\n",
       " 'a',\n",
       " ',',\n",
       " 'but',\n",
       " ',',\n",
       " 'xxmaj',\n",
       " '.',\n",
       " 'xxmaj',\n",
       " 'xxmaj',\n",
       " 'it',\n",
       " 'is',\n",
       " 'is',\n",
       " 'the',\n",
       " 'level',\n",
       " 'is',\n",
       " 'not',\n",
       " '.',\n",
       " '.',\n",
       " 'and',\n",
       " 'suicide',\n",
       " ',',\n",
       " 'xxmaj',\n",
       " 'film',\n",
       " 'is',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'is',\n",
       " 'that',\n",
       " 'the',\n",
       " 'a',\n",
       " 'xxmaj',\n",
       " '.',\n",
       " 'xxmaj',\n",
       " 'xxmaj',\n",
       " 'where',\n",
       " 'made',\n",
       " 'in',\n",
       " 'the',\n",
       " 'xxmaj',\n",
       " ',',\n",
       " ',',\n",
       " 'is',\n",
       " 'out',\n",
       " 'to',\n",
       " 'a',\n",
       " 'a',\n",
       " 'xxmaj',\n",
       " 'xxunk',\n",
       " 'man',\n",
       " 'he',\n",
       " 'that',\n",
       " 'of',\n",
       " 'that',\n",
       " 'knows',\n",
       " 'is',\n",
       " '.',\n",
       " 'xxmaj',\n",
       " 'xxmaj',\n",
       " 'xxmaj',\n",
       " 'who',\n",
       " 'xxmaj',\n",
       " 'joseph',\n",
       " 'xxmaj',\n",
       " 'xxunk',\n",
       " 'is',\n",
       " 'xxunk',\n",
       " 'is',\n",
       " 'xxmaj',\n",
       " 'xxunk',\n",
       " 'xxmaj',\n",
       " 'scott',\n",
       " ')',\n",
       " 'is',\n",
       " 'the',\n",
       " 'plot',\n",
       " '.',\n",
       " 'with',\n",
       " 'xxmaj',\n",
       " 'son',\n",
       " 'xxmaj',\n",
       " 'joseph',\n",
       " 'xxmaj',\n",
       " 'reed',\n",
       " 'and',\n",
       " 'xxmaj',\n",
       " 'ann',\n",
       " 'xxmaj',\n",
       " 'xxunk',\n",
       " ')',\n",
       " '.',\n",
       " 'xxmaj',\n",
       " 'the',\n",
       " 'xxmaj',\n",
       " 'a',\n",
       " 'nice',\n",
       " 'who',\n",
       " 'xxmaj',\n",
       " 'ann',\n",
       " 'does',\n",
       " '.',\n",
       " 'is',\n",
       " 'a',\n",
       " 'any',\n",
       " 'name',\n",
       " ',',\n",
       " '.',\n",
       " 'on',\n",
       " 'their',\n",
       " '-',\n",
       " '-',\n",
       " '.',\n",
       " 'xxmaj',\n",
       " 'to',\n",
       " 'the',\n",
       " '.',\n",
       " 'xxmaj',\n",
       " 'xxmaj',\n",
       " 'the',\n",
       " ...]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preds.cpu().apply_(denumericalize)\n",
    "list(map(denumericalize, preds.flatten().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(learn.model.state_dict(), './language_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
