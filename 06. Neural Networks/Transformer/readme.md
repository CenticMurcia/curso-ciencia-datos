<h1 align="center">Transformer</h1>

![](img/Attention.png)

## Better initialization: [T-Fixup](http://www.cs.toronto.edu/~mvolkovs/ICML2020_tfixup.pdf)

## Theory Reference

- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) (Jay Alammar)
- Towards Data Science: Transformers Explained Visually
  - [Part 1: Overview of Functionality](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452)
  - [Part 2: How it works, step-by-step](https://towardsdatascience.com/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34)


## Code Refernce

- Low Level code (from scratch): Useful for understand the transformer and make custom changes
  - [Transformers from Scratch](http://peterbloem.nl/blog/transformers)
  - [How to code The Transformer in Pytorch (Towards Data Science)](https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec)
  - [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) (Harvardâ€™s NLP group)
  - [Dive into Deep Learning: Transformer chapter](https://d2l.ai/chapter_attention-mechanisms/transformer.html)
- High Level code: Usful for quick use and train
  - [Simple PyTorch-Lightning Transformer Example with Greedy Decoding](https://colab.research.google.com/drive/1swXWW5sOLW8zSZBaQBYcGQkQ_Bje_bmI)
