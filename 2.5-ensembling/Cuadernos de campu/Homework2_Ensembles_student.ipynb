{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6eAsdbmD4uOX"
   },
   "source": [
    "# Week 6: Homework 2 \n",
    "\n",
    "----------------------------------------------------\n",
    "Machine Learning                      \n",
    "\n",
    "Year 2020/2021\n",
    "\n",
    "*Vanessa Gómez Verdejo vanessa@tsc.uc3m.es* \n",
    "\n",
    "----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mQfy2KvsqU3u"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lgDm0Z5bcazI"
   },
   "source": [
    "The aim of this second HW is to implement and analyse the performance of ensemble methods. To do this, we will work with the Breast Cancer database (described in the next section) and you will have to complete the following exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpjaOvF7Ldhq"
   },
   "source": [
    "## Exercise 1. Load and prepare the data \n",
    "\n",
    "For this lab session, let's work over the  [Breast cancer data set](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html) a binary classification problem aimed to detect breast cancer from a  digitalized image of breast mass. For this purpose, the images have been preprocesed and characterized with 30 input features describing the mass.\n",
    "\n",
    "Complete next cell code, so that you can:\n",
    "* Load the dataset\n",
    "* Create training and testing partitions with the 75% and 25% of the original data\n",
    "* Normalize the data to zero mean and unitary standard deviation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b60ScGu0B5K0",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6c53497f09e264b8",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "#### Include your answer here!\n",
    "#### Create as many cells (code or markdown) as you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2szGWImcK10A"
   },
   "source": [
    "## Exercise 2. Bagging methods (5.5 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0TL6ZABd9ES"
   },
   "source": [
    "### Exercise 2.1  (1 point)\n",
    "\n",
    "Use the model BaggingClassifier of sklearn to train an ensemble of T trees with a subsamplig rate of 50% (`max_samples` = 0.5). For this exercise, consider that each tree has a maximum depth of 3 and use the default values for the remaining parameters.\n",
    "\n",
    "Analyze the evolution of the train and test accuracy for T from 1 to 50."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzsSkiiUCBHK",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-aa792425bf52e1ff",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "#### Include your answer here!\n",
    "#### Create as many cells (code or markdown) as you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKqY5xhUd7Yp"
   },
   "source": [
    "### Exercise 2.2 Influence of parameter `n_perc` and ensemble diversity (1.5 points)\n",
    "\n",
    "For the above ensemble (with T=50), analyze the behaviour of the ensemble for different values of `max_samples` (from 5% to 100%) and analyze the performances over the train and test partitions. Compare them with that of a single decision tree (with `max_depth` of 3) also trained with `max_samples` samples. For both approaches, run several iterations and average the results to obtain representative performance curves.\n",
    "\n",
    "Analyze the results and answer the following questions:\n",
    "* What is the advantage of the ensemble compared to a stand-alone tree? \n",
    "\n",
    "* Is this advantage equal for any value of `max_samples`?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lxi52vm2CCqV",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3000f503f70076b0",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "#### Include your answer here!\n",
    "#### Create as many cells (code or markdown) as you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qHKAOerp18u"
   },
   "source": [
    "Now analyze the **diversity** among their base learners' outputs for different `max_samples` rates. You can analyze this diversity by measuring the correlation among the learners' outputs. That is, once you have trained an ensemble for a `perc` value, you can compute this diversity as:\n",
    "1. Compute the learner soft-outputs (over the training data): use the method `.predict_proba()` of the decision trees. Note that the ensemble has an attribute (`estimators_`) with all the learned trees.\n",
    "2. Obtain the matrix with all pairwaise correlation values (you can use [`np.corrcoef`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.corrcoef.html))\n",
    "3. Compute the ensemble diversity as one minus the averaged value of all pairwise correlation values. \n",
    "\n",
    "Finally, analyze the results. Which is better, a high or low diversity? To answer this question, it can be interesting to jointly analyze the diversity and the ensemble accuracy for different `max_samples` values. And do not forget to average your results for different runs.\n",
    "\n",
    "\n",
    "Note: You can find other ensemble diversity measurements in https://lucykuncheva.co.uk/papers/lkml.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hm1S7A8yCGgE",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0d02283b3f52c1ad",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "#### Include your answer here!\n",
    "#### Create as many cells (code or markdown) as you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LoL9dedzh0Q6"
   },
   "source": [
    "### Exercise 2.3 Can we increase the ensemble diversity with other schemes? (1 point)\n",
    "\n",
    "A very simple way to increase diversity is to do sample and feature selection at the same time (subsampling the training data matrix by rows and by columns). To do this, the `BaggingClassifier` class, just as it has the `max_samples` parameter to indicate the number or percentage of training samples to use, has another parameter `max_features` that allows to control the number or percentage of variables to use.\n",
    "\n",
    "To analyze the influence of this kind of subsampling, fix `max_samples` to 0.5 and analyze the ensemble performance and diversity for different values of `max_features`. Discuss the obtained results in comparison with those of Exercise 2.2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPOr_rrmCINM",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-de27bd13b2568ca5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "#### Include your answer here!\n",
    "#### Create as many cells (code or markdown) as you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILkeQIyk1sm3"
   },
   "source": [
    "### Exercise 2.4 (1 point)\n",
    "\n",
    "Now compare the performace of the bagged ensemble of Exercise 2.1 (using only sample subsampling) with that of a Random Forest and a Extremely Randomized Tree. Use the same number of learners (T=50), and also use decision trees with a maximum depth of 3, but crossvalidate the subsampling rates (note that RF has to CV the number of samples and features).\n",
    "\n",
    "Which differences are among these methods? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be0XDLaACJrI",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c08f6fe6f0cceed7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "#### Include your answer here!\n",
    "#### Create as many cells (code or markdown) as you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MZ2q3Adn4Wz"
   },
   "source": [
    "### Exercise 2.5  (1 point)\n",
    "Both `RandomForestClassifier()` and `ExtraTreesClassifier()` are able to learn during their training the **feature relevances**. Analyze the feature importances provided by them. Do they agree? Do they provide relevant information about the Breast Cancer detection?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BnqwP78QCK_H",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a729ece51c7d78a4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "#### Include your answer here!\n",
    "#### Create as many cells (code or markdown) as you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQtVXIQyOAEP"
   },
   "source": [
    "## Exercise 3.Boosting methods (3.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "debDlxM_scrH"
   },
   "source": [
    "### Exercise 3.1  (1 point)\n",
    "\n",
    "\n",
    "Uses the `AdaBoostClassifier` model to train a set of 50 decision trees with depth 3 and analyze their train and test accuracy vs. the number of trees (from 1 to 50). Use the two implementations of Adaboost (`algorithm` = `SAMME` and `algorithm` = `SAMME.R`) and compare the result with the set constructed by Bagging in Exercise 2.1.\n",
    "\n",
    "Discuss the results and answer the following questions:\n",
    "* Can I stop adding learners when the train accuracy is $100\\%$?\n",
    "* Can I add as many trees as I want without incurring in overfitting problems? Could I do the same with a bagging ensemble?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMTme1OCCMyx",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-87e7add9627db0b0",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "#### Include your answer here!\n",
    "#### Create as many cells (code or markdown) as you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nCkOg5rs012"
   },
   "source": [
    "### Exercise 3.2  (1 point)\n",
    "\n",
    "Use the `GradientBoostingClassifier` model to train a set of 50 decision trees with depth 3 by Gradient Boosting with exponential (`loss= 'exponential'`) and binomial (`loss= 'deviance'`) cost function. In both cases, obtain the accuracy evolution with the number of learners. Besides, compare the result with the one obtained by the `AdaBoostClassifier` with `algorithm` = `SAMME.R`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15nDpV3KCPj9",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-221533cb07900ceb",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "#### Include your answer here!\n",
    "#### Create as many cells (code or markdown) as you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b44mKOUBu_ih"
   },
   "source": [
    "### Exercise 3.3 (1.5 points)\n",
    "\n",
    "Finally compare the performance of GradientBoost based models when using the \n",
    "[XGBoost library](https://xgboost.readthedocs.io/en/latest/index.html), as it provides an efficient implementation with parallelization capabilities (in case we need to work with large datasets). In addition, it has a [sklearn interface](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn) that allows us to easily integrate it with our code.\n",
    "\n",
    "Like the sklearn implementation it is designed to work with decision trees, but it brings us some additional utilities:\n",
    "* It allows us to do *early_stopping*: if we provide a validation set, it evaluates the performance of the set on it to decide when to stop adding new trees (avoiding overfitting effects).\n",
    "* It allows to evaluate the set with a subset of the trained trees.\n",
    "\n",
    "Use these utilities to efficiently analyze the performance of the ensemble as a function of the number of trees and apply an early stopping criterion to select the optimal number of trees in the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "faS-kazBCQjg",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fd480a514ff4f780",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "#### Include your answer here!\n",
    "#### Create as many cells (code or markdown) as you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8IW-mLi_NEM"
   },
   "source": [
    "### Exercise 4. Stacking of classifiers (1 point)\n",
    "\n",
    "Using the stacking scheme, select different base classifiers from those seen in class and perform a combination of them at different levels. Justify the selection of the base classifiers, as well as the classifier used at the output. Compare the performances with those of the ensembles designed in the previous sections, discuss their advantages and disadvantages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ITj2dB3CSbK",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-51bccad7bbd454af",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "#### Include your answer here!\n",
    "#### Create as many cells (code or markdown) as you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "colab": {
   "collapsed_sections": [],
   "name": "Homework2_Ensembles_student.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
