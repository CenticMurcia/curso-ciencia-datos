---
layout: page

parent_id: 2.1-tree
id: 4-hyperparams
title: ğŸ”§ Hyperparameters
---


|           | sklearn<br>Random Forest | XGBoost<br>Gradient Boosting | LightGBM<br>Gradient Boosting | Try |
|--------------------------------------|:--------------------:|:----------------:|:----------------:|-------------|
| ğŸ”· Number of trees                   | N_estimators         | num_round ğŸ’¡     | num_iterations ğŸ’¡| [10,...,1000] |
| ğŸ”· Max depth of the tree             | max_depth            | max_depth        | max_depth        | 3,...,10    |
| ğŸ”¶ Min cases per final tree leaf     | min_samples_leaf     | min_child_weight | min_data_in_leaf |             |
| ğŸ”· % of rows used to build the tree  | max_samples          | subsample        | bagging_fraction | 0.8         |
| ğŸ”· % of feats used to build the tree | max_features         | colsample_bytree | feature_fraction |             |
| ğŸ”· Speed of training                 | NOT IN FOREST        | eta              | learning_rate    |             |
| ğŸ”¶ L1 regularization                 | NOT IN FOREST        | lambda           | lambda_l1        |             |
| ğŸ”¶ L2 regularization                 | NOT IN FOREST        | alpha            | lambda_l2        |             |
| Random seed                          | random_state         | seed             | _seed            |             |


> - ğŸ”·: Increase parameter for overfit,  decrease for underfit.
> - ğŸ”¶: Increase parameter for underfit, decrease for overfit. (regularization)
> - ğŸ’¡: For Gradient Boosting maybe is better to do early stopping rather than set a fixed number of trees.