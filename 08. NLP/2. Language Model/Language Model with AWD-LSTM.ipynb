{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: We need preprocessed texts (tokinized and numericalized) in this notebook\n",
    "\n",
    "> Notebook based on:\n",
    "> 1. https://github.com/fastai/course-v3/blob/master/nbs/dl2/12_text.ipynb\n",
    "> 2. https://github.com/fastai/course-v3/blob/master/nbs/dl2/12a_awd_lstm.ipynb\n",
    "> 3. https://github.com/fastai/course-v3/blob/master/nbs/dl2/12b_lm_pretrain.ipynb\n",
    "> 4. https://github.com/fastai/course-v3/blob/master/nbs/dl2/12c_ulmfit.ipynb\n",
    "> \n",
    "> Video:\n",
    "> - https://youtu.be/vnOpEwmtFJ8?t=4687 from 1:18:00 to 2:08:00 (50 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pathlib\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test  train  unsup  vocab.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls \"../../Datasets/NLP/IMBd_prepro\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg  pos\r\n"
     ]
    }
   ],
   "source": [
    "!ls \"../../Datasets/NLP/IMBd_prepro/train\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "def parallel_map(func, array):\n",
    "    \n",
    "    cpu_cores = multiprocessing.cpu_count()\n",
    "    array_len = len(array)\n",
    "    chunksize = array_len // 100\n",
    "    \n",
    "    if cpu_cores<2:\n",
    "        return list(tqdm(map(func, arr), total=array_len))\n",
    "    else:\n",
    "        with ProcessPoolExecutor(max_workers=cpu_cores) as ex:\n",
    "            return list(tqdm(ex.map(func, array, chunksize=chunksize), total=array_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <center> Dataset & Dataloader for Langauge Model\n",
    "- X: Text\n",
    "- Y: Same text but shifted by 1 token\n",
    "\n",
    "At every epoch:\n",
    "\n",
    "1. **Shuffle** (sort randomly) our collection of texts.\n",
    "2. **Concatenate** the individual texts together into a big stream. \n",
    "3. **Cut** this stream into a certain number of batches (which is our batch size).\n",
    "   - For instance, if the stream has 50,000 tokens and we set a batch size of 10, this will give us 10 mini-streams of 5,000 tokens.\n",
    "   \n",
    "So to recap, at every epoch we shuffle our collection of documents and concatenate them into a stream of tokens. We then cut that stream into a batch of fixed-size consecutive mini-streams. Our model will then read the mini-streams in order, and thanks to an inner state, it will produce the same activation whatever sequence length we picked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, prepro_texts_dir, bptt=70, shuffle=False):\n",
    "        \n",
    "        # Read tokenized and numeralized text files (numpy format)\n",
    "        np_filepaths = list( pathlib.Path(prepro_texts_dir).glob('**/*.npy') ) \n",
    "        \n",
    "        # Open numpy arrays in parallel\n",
    "        self.texts_np = parallel_map(func=np.load, array=np_filepaths)\n",
    "\n",
    "        self.bptt    = bptt\n",
    "        self.shuffle = shuffle\n",
    "        self.total_tokens = sum([len(t) for t in self.texts_np])\n",
    "        \n",
    "        self.concat_texts_into_stream()\n",
    "        \n",
    "    # this is necesseary at the begining of every epoch for train !!!!\n",
    "    def concat_texts_into_stream(self):\n",
    "        \n",
    "        # 1. Reorder texts if we need to\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.texts_np)\n",
    "            #self.texts_np = self.texts_np[np.random.permutation(len(self.texts_np))]\n",
    "            \n",
    "        # 2. Concat texts into a large stream\n",
    "        self.stream = np.concatenate(self.texts_np)\n",
    "        #self.stream = torch.cat([torch.Tensor(t) for t in self.texts_np])\n",
    "                \n",
    "    def __len__(self):\n",
    "        return self.total_tokens // self.bptt\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.stream[idx   : idx+self.bptt]\n",
    "        y = self.stream[idx+1 : idx+self.bptt+1] # shifted by 1\n",
    "        \n",
    "        # convert from numpy.uint16 to torch.int64        \n",
    "        x = torch.tensor(x.astype(\"int64\"))\n",
    "        y = torch.tensor(y.astype(\"int64\"))\n",
    "        \n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca6d9f4fb4942f7b01b7c25b05a7d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a6cd379b484a6986cf58985d21ef0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BPTT_LEN = 70 # Lengh of the minisequences in the big stream\n",
    "\n",
    "train_ds = LM_Dataset(\"../../Datasets/NLP/IMBd_prepro/train\", bptt=BPTT_LEN, shuffle=True)\n",
    "valis_ds = LM_Dataset(\"../../Datasets/NLP/IMBd_prepro/test\",  bptt=BPTT_LEN, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([    2,    18,   160,   476,    19,    29,   144,   251,    16,   106,\n",
       "           410,    60,     9,    18,    25,    59,  2837,   174,   176,    10,\n",
       "            11,   166,    18,   167,  8772,     9,    18,   419,   416,     8,\n",
       "           397,   473,    34,     7, 25389,     7, 61636,     9,    18,    85,\n",
       "            52,    94,   267,    10,    18,    73,   133,    14,   142,    77,\n",
       "           480,    19,    29,     9,     7,    39,  1200,     7,   536,     7,\n",
       "          1712,    26,    88,    11,    18,   140,    39,   105,   133,    19]),\n",
       " tensor([   18,   160,   476,    19,    29,   144,   251,    16,   106,   410,\n",
       "            60,     9,    18,    25,    59,  2837,   174,   176,    10,    11,\n",
       "           166,    18,   167,  8772,     9,    18,   419,   416,     8,   397,\n",
       "           473,    34,     7, 25389,     7, 61636,     9,    18,    85,    52,\n",
       "            94,   267,    10,    18,    73,   133,    14,   142,    77,   480,\n",
       "            19,    29,     9,     7,    39,  1200,     7,   536,     7,  1712,\n",
       "            26,    88,    11,    18,   140,    39,   105,   133,    19,    29]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('xxbos xxmaj the choice to make this snl skit into a movie was far better thought out than other recent ones . xxmaj the humor involved in the character is not annoyance humor , and is also character driven enough to be stretched out for an hour or two . \\n\\n xxmaj oddly enough the sexual content seemed like it could be avoided , but that may have been because',\n",
       " 'xxmaj the choice to make this snl skit into a movie was far better thought out than other recent ones . xxmaj the humor involved in the character is not annoyance humor , and is also character driven enough to be stretched out for an hour or two . \\n\\n xxmaj oddly enough the sexual content seemed like it could be avoided , but that may have been because the')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\" \".join(denumericalize(ds[0][0])), \" \".join(denumericalize(ds[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader (with custom sampler for BPTT)\n",
    "\n",
    "if we divide our big stream of **28 elements** with **batch_size of 5**:\n",
    "\n",
    "|               |              |               |               |               |               |\n",
    "|---------------|--------------|---------------|---------------|---------------|---------------|\n",
    "| **1st batch** | stream_idx 0 | stream_idx 6  | stream_idx 12 | stream_idx 18 | stream_idx 23 |\n",
    "| **2nd batch** | stream_idx 1 | stream_idx 7  | stream_idx 13 | stream_idx 19 | stream_idx 24 |\n",
    "| **3rd batch** | stream_idx 2 | stream_idx 8  | stream_idx 14 | stream_idx 20 | stream_idx 25 |\n",
    "| **4th batch** | stream_idx 3 | stream_idx 8  | stream_idx 15 | stream_idx 21 | stream_idx 26 |\n",
    "| **5th batch** | stream_idx 4 | stream_idx 10 | stream_idx 16 | stream_idx 22 | stream_idx 27 |\n",
    "| **6th batch** | stream_idx 5 | stream_idx 11 | stream_idx 17 |               |               |\n",
    "\n",
    "https://pytorchnlp.readthedocs.io/en/latest/_modules/torchnlp/samplers/bptt_sampler.html\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPTT_BatchSampler(Sampler):\n",
    "    def __init__(self, n_elements, batch_size, drop_last):\n",
    "        \n",
    "        indexes = np.array_split(list(range(n_elements)), batch_size) # magic happens here\n",
    "        \n",
    "        n_batches = n_elements//batch_size\n",
    "        self.batches_idxs = np.array([x[:n_batches] for x in indexes]).T.tolist()\n",
    "        \n",
    "        if not drop_last:\n",
    "            last_batch_idxs = np.array([x[n_batches:] for x in indexes if x[n_batches:].size==1]).T.tolist()\n",
    "            self.batches_idxs += last_batch_idxs\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return iter(self.batches_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 6, 12, 18, 23],\n",
       " [1, 7, 13, 19, 24],\n",
       " [2, 8, 14, 20, 25],\n",
       " [3, 9, 15, 21, 26],\n",
       " [4, 10, 16, 22, 27],\n",
       " [5, 11, 17]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = BPTT_BatchSampler(n_elements=28, batch_size=5, drop_last=False)\n",
    "list(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 6, 12, 18, 23],\n",
       " [1, 7, 13, 19, 24],\n",
       " [2, 8, 14, 20, 25],\n",
       " [3, 9, 15, 21, 26],\n",
       " [4, 10, 16, 22, 27]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = BPTT_BatchSampler(n_elements=28, batch_size=5, drop_last=True)\n",
    "list(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_sampler=BPTT_BatchSampler(n_elements=len(train_ds),\n",
    "                                                                batch_size=BATCH_SIZE,\n",
    "                                                                drop_last=True))\n",
    "\n",
    "vaalid_dl = DataLoader(valid_ds, batch_sampler=BPTT_BatchSampler(n_elements=len(valid_ds),\n",
    "                                                                batch_size=BATCH_SIZE,\n",
    "                                                                drop_last=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
