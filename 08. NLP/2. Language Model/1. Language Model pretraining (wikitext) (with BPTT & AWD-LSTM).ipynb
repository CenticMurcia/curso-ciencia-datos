{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: We need preprocessed texts (tokinized and numericalized) in this notebook\n",
    "\n",
    "> Notebook based on:\n",
    "> 1. https://github.com/fastai/course-v3/blob/master/nbs/dl2/12_text.ipynb\n",
    "> 2. https://github.com/fastai/course-v3/blob/master/nbs/dl2/12a_awd_lstm.ipynb\n",
    "> 3. https://github.com/fastai/course-v3/blob/master/nbs/dl2/12b_lm_pretrain.ipynb\n",
    "> 4. https://github.com/fastai/course-v3/blob/master/nbs/dl2/12c_ulmfit.ipynb\n",
    "> \n",
    "> Video:\n",
    "> - https://youtu.be/vnOpEwmtFJ8?t=4687 from 1:18:00 to 2:08:00 (50 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "BPTT_LEN   = 70 # Lengh of the minisequences in the big stream (used In Dataset)\n",
    "BATCH_SIZE = 64 # (used In DataLoader)\n",
    "PAD_TOKEN  = 1  # Number for xxPad token (used in the nn.Embedding layer of the Model)\n",
    "VOCAB_LEN  = None # This will be updated when we read the vocab.pkl file (used in the nn.Embedding layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch: 1.8.1\n",
      "Fast.ai: 2.2.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pathlib\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, Sampler, DataLoader\n",
    "\n",
    "import fastai\n",
    "print(\"Pytorch:\", torch.__version__)\n",
    "print(\"Fast.ai:\", fastai.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test  train  unsup  vocab.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls \"../../Datasets/NLP/IMBd_prepro\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg  pos\r\n"
     ]
    }
   ],
   "source": [
    "!ls \"../../Datasets/NLP/IMBd_prepro/train\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = open(\"../../Datasets/NLP/IMBd_prepro/vocab.pkl\",'rb')\n",
    "vocab = pickle.load(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65539"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_LEN = len(vocab)\n",
    "VOCAB_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denumericalize(list_of_nums):\n",
    "    return [vocab[i] for i in list_of_nums]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxunk',\n",
       " 'xxpad',\n",
       " 'xxbos',\n",
       " 'xxeos',\n",
       " 'xxrep',\n",
       " 'xxwrep',\n",
       " 'xxup',\n",
       " 'xxmaj',\n",
       " 'the',\n",
       " '.',\n",
       " ',',\n",
       " 'and',\n",
       " 'a',\n",
       " 'of',\n",
       " 'to',\n",
       " 'is',\n",
       " 'it',\n",
       " 'in',\n",
       " 'i',\n",
       " 'this']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denumericalize( range(20) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "def parallel_map(func, array):\n",
    "    \n",
    "    cpu_cores = multiprocessing.cpu_count()\n",
    "    array_len = len(array)\n",
    "    chunksize = array_len // 100\n",
    "    \n",
    "    if cpu_cores<2:\n",
    "        return list(tqdm(map(func, arr), total=array_len))\n",
    "    else:\n",
    "        with ProcessPoolExecutor(max_workers=cpu_cores) as ex:\n",
    "            return list(tqdm(ex.map(func, array, chunksize=chunksize), total=array_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <center> Dataset & Dataloader for Langauge Model\n",
    "- X: Text\n",
    "- Y: Same text but shifted by 1 token\n",
    "\n",
    "At every epoch:\n",
    "\n",
    "1. **Shuffle** (sort randomly) our collection of texts.\n",
    "2. **Concatenate** the individual texts together into a big stream. \n",
    "3. **Cut** this stream into a certain number of batches (which is our batch size).\n",
    "   - For instance, if the stream has 50,000 tokens and we set a batch size of 10, this will give us 10 mini-streams of 5,000 tokens.\n",
    "   \n",
    "So to recap, at every epoch we shuffle our collection of documents and concatenate them into a stream of tokens. We then cut that stream into a batch of fixed-size consecutive mini-streams. Our model will then read the mini-streams in order, and thanks to an inner state, it will produce the same activation whatever sequence length we picked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, prepro_texts_dir, bptt=70, shuffle=False):\n",
    "        \n",
    "        # Read tokenized and numeralized text files (numpy format)\n",
    "        np_filepaths = list( pathlib.Path(prepro_texts_dir).glob('**/*.npy') ) \n",
    "        \n",
    "        # Open numpy arrays in parallel\n",
    "        #self.texts_np = list(map(func=np.load, np_filepaths))  # Non parallel version   \n",
    "        self.texts_np = parallel_map(func=np.load, array=np_filepaths)\n",
    "\n",
    "        self.bptt    = bptt\n",
    "        self.shuffle = shuffle\n",
    "        self.total_tokens = sum([len(t) for t in self.texts_np])\n",
    "        \n",
    "        self.concat_texts_into_stream()\n",
    "        \n",
    "    # this is necesseary at the begining of every epoch for train !!!!\n",
    "    def concat_texts_into_stream(self):\n",
    "        \n",
    "        # 1. Reorder texts if we need to\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.texts_np)\n",
    "            #self.texts_np = self.texts_np[np.random.permutation(len(self.texts_np))]\n",
    "            \n",
    "        # 2. Concat texts into a large stream\n",
    "        self.stream = np.concatenate(self.texts_np)\n",
    "        #self.stream = torch.cat([torch.Tensor(t) for t in self.texts_np])\n",
    "                \n",
    "    def __len__(self):\n",
    "        return self.total_tokens // self.bptt\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.stream[idx   : idx+self.bptt]\n",
    "        y = self.stream[idx+1 : idx+self.bptt+1] # shifted by 1\n",
    "        \n",
    "        # convert from numpy.uint16 to torch.int64        \n",
    "        x = torch.tensor(x.astype(\"int64\"))\n",
    "        y = torch.tensor(y.astype(\"int64\"))\n",
    "        \n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ee5b45b65c416bb679f4c118c047c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7239d5fbc686474ba5966e437091df09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_ds = LM_Dataset(\"../../Datasets/NLP/IMBd_prepro/train\", bptt=BPTT_LEN, shuffle=True)\n",
    "valid_ds = LM_Dataset(\"../../Datasets/NLP/IMBd_prepro/test\",  bptt=BPTT_LEN, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   18,   235,    19,    29,     8,   106,  1622,     8,   578,    13,\n",
       "         55660,   571,    89,    34,    14,     8,  1325,    23,    88,    18,\n",
       "          5480,     8,   219,  6021,  3825,    17,     8,   475,    10,    30,\n",
       "             8,  1900,   469,    16,    11,     8,     0,   308,    13,   218,\n",
       "          8574,   171,    23,    18,    58,    37,   140,    49,   276,     8,\n",
       "           210,    57,    23,    30,    71,    25,   101,    11,    52,    25,\n",
       "          6696,    50,    50,    50,    23,    24,     8,   127,   102,   100]),\n",
       " tensor([  235,    19,    29,     8,   106,  1622,     8,   578,    13, 55660,\n",
       "           571,    89,    34,    14,     8,  1325,    23,    88,    18,  5480,\n",
       "             8,   219,  6021,  3825,    17,     8,   475,    10,    30,     8,\n",
       "          1900,   469,    16,    11,     8,     0,   308,    13,   218,  8574,\n",
       "           171,    23,    18,    58,    37,   140,    49,   276,     8,   210,\n",
       "            57,    23,    30,    71,    25,   101,    11,    52,    25,  6696,\n",
       "            50,    50,    50,    23,    24,     8,   127,   102,   100,    49]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([70])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('xxbos i saw this movie the first seconds the voice of t.r. took me on to the journey - well i disliked the big glued thumbs in the beginning , but the absurd humor it and the xxunk looks of both sissy actors - i do not know who played the young her - but she was great and so was uma ! ! ! - \\n\\n the two other',\n",
       " 'i saw this movie the first seconds the voice of t.r. took me on to the journey - well i disliked the big glued thumbs in the beginning , but the absurd humor it and the xxunk looks of both sissy actors - i do not know who played the young her - but she was great and so was uma ! ! ! - \\n\\n the two other people')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(denumericalize(train_ds[0][0])), \" \".join(denumericalize(train_ds[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader (with custom sampler for BPTT)\n",
    "\n",
    "if we divide our big stream of **28 elements** with **batch_size of 5**:\n",
    "\n",
    "|               |              |               |               |               |               |\n",
    "|---------------|--------------|---------------|---------------|---------------|---------------|\n",
    "| **1st batch** | stream_idx 0 | stream_idx 6  | stream_idx 12 | stream_idx 18 | stream_idx 23 |\n",
    "| **2nd batch** | stream_idx 1 | stream_idx 7  | stream_idx 13 | stream_idx 19 | stream_idx 24 |\n",
    "| **3rd batch** | stream_idx 2 | stream_idx 8  | stream_idx 14 | stream_idx 20 | stream_idx 25 |\n",
    "| **4th batch** | stream_idx 3 | stream_idx 8  | stream_idx 15 | stream_idx 21 | stream_idx 26 |\n",
    "| **5th batch** | stream_idx 4 | stream_idx 10 | stream_idx 16 | stream_idx 22 | stream_idx 27 |\n",
    "| **6th batch** | stream_idx 5 | stream_idx 11 | stream_idx 17 |               |               |\n",
    "\n",
    "https://pytorchnlp.readthedocs.io/en/latest/_modules/torchnlp/samplers/bptt_sampler.html\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPTT_BatchSampler(Sampler):\n",
    "    def __init__(self, n_elements, batch_size, drop_last):\n",
    "        \n",
    "        indexes = np.array_split(list(range(n_elements)), batch_size) # magic happens here\n",
    "        \n",
    "        n_batches = n_elements//batch_size\n",
    "        self.batches_idxs = np.array([x[:n_batches] for x in indexes]).T.tolist()\n",
    "        \n",
    "        if not drop_last:\n",
    "            last_batch_idxs = np.array([x[n_batches:] for x in indexes if x[n_batches:].size==1]).T.tolist()\n",
    "            self.batches_idxs += last_batch_idxs\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return iter(self.batches_idxs)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.batches_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,\n",
       " [[0, 6, 12, 18, 23],\n",
       "  [1, 7, 13, 19, 24],\n",
       "  [2, 8, 14, 20, 25],\n",
       "  [3, 9, 15, 21, 26],\n",
       "  [4, 10, 16, 22, 27],\n",
       "  [5, 11, 17]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = BPTT_BatchSampler(n_elements=28, batch_size=5, drop_last=False)\n",
    "len(s), list(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " [[0, 6, 12, 18, 23],\n",
       "  [1, 7, 13, 19, 24],\n",
       "  [2, 8, 14, 20, 25],\n",
       "  [3, 9, 15, 21, 26],\n",
       "  [4, 10, 16, 22, 27]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = BPTT_BatchSampler(n_elements=28, batch_size=5, drop_last=True)\n",
    "len(s), list(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 70]), torch.Size([64, 70]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dl = DataLoader(train_ds, batch_sampler=BPTT_BatchSampler(n_elements=len(train_ds),\n",
    "                                                                batch_size=BATCH_SIZE,\n",
    "                                                                drop_last=True))\n",
    "\n",
    "valid_dl = DataLoader(valid_ds, batch_sampler=BPTT_BatchSampler(n_elements=len(valid_ds),\n",
    "                                                                batch_size=BATCH_SIZE,\n",
    "                                                                drop_last=True))\n",
    "\n",
    "batch_x, batch_y = next(iter(train_dl))\n",
    "batch_x.shape, batch_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Simple LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "     \n",
    "    def __init__(self, vocab_sz, emb_dim, hidden_dim, n_layers):\n",
    "        super().__init__()\n",
    "                \n",
    "        # Embeding\n",
    "        self.emb    = nn.Embedding(vocab_sz, emb_dim, padding_idx=PAD_TOKEN)\n",
    "        emb_init_range = 0.1\n",
    "        self.emb.weight.data.uniform_(-emb_init_range, emb_init_range)\n",
    "        \n",
    "        # LSTMs\n",
    "        self.lstm = nn.LSTM(input_size  = emb_dim,\n",
    "                            hidden_size = hidden_dim,\n",
    "                            num_layers  = n_layers,\n",
    "                            batch_first = True)\n",
    "        self.in_hidden = [torch.zeros(n_layers, BATCH_SIZE, hidden_dim) for _ in range(2)] # Initial hidden state\n",
    "        \n",
    "        # Linear\n",
    "        self.head = nn.Linear(in_features=hidden_dim, out_features=vocab_sz, bias=True)\n",
    "        \n",
    "    def forward(self, input_seq):\n",
    "        out = self.emb(input_seq)\n",
    "        out, out_hidden = self.lstm(out, self.in_hidden)\n",
    "        out = self.head(out)\n",
    "        \n",
    "        self.in_hidden = [h.detach() for h in out_hidden] # Truncated BPTT\n",
    "        return out\n",
    "    \n",
    "    def reset(self):\n",
    "        for h in self.in_hidden: h.zero_()\n",
    "            \n",
    "    # Move initial hidden state to CUDA when model.to(device)\n",
    "    def to(self, *args, **kwargs):\n",
    "        self = super().to(*args, **kwargs)\n",
    "        self.in_hidden = [h.to(*args, **kwargs) for h in self.in_hidden]\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 70, 65539])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test model\n",
    "model = SimpleLSTM(vocab_sz=VOCAB_LEN, emb_dim=300, hidden_dim=700, n_layers=2)\n",
    "#tst_input = torch.randint(low=0,high=100, size=(64,70))\n",
    "\n",
    "batch_p = model(batch_x) # prediction\n",
    "batch_p.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss: Flat CrossEntropy\n",
    "\n",
    "Custom CrossEntropyLossFlat inspired by [Fast.ai CrossEntropyLossFlat()](https://docs.fast.ai/losses.html#CrossEntropyLossFlat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred:   torch.Size([64, 70, 65539])\n",
      "Target: torch.Size([64, 70])\n"
     ]
    }
   ],
   "source": [
    "print(\"Pred:  \", batch_p.shape)\n",
    "print(\"Target:\", batch_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred:   torch.Size([4480, 65539])\n",
      "Target: torch.Size([4480])\n"
     ]
    }
   ],
   "source": [
    "batch_p_flatten = batch_p.view(-1, batch_p.shape[-1])\n",
    "batch_y_flatten = batch_y.view(-1)\n",
    "\n",
    "print(\"Pred:  \", batch_p_flatten.shape)\n",
    "print(\"Target:\", batch_y_flatten.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLossFlat(nn.Module):\n",
    "    \"\"\"Flatten version of nn.CrossEntropyLoss()\"\"\"\n",
    "    def forward(self, pred, target):\n",
    "        pred_flatten   = pred.view(-1, pred.shape[-1])\n",
    "        target_flatten = target.view(-1)\n",
    "        return F.cross_entropy(pred_flatten, target_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.0838, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "ce_loss(batch_p_flatten, batch_y_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.0838, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ce_flat_loss = CrossEntropyLossFlat()\n",
    "ce_flat_loss(batch_p, batch_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fastai.text.all import *\n",
    "from fastai.learner           import Learner\n",
    "from fastai.data.core         import DataLoaders\n",
    "from fastai.metrics           import accuracy, Perplexity\n",
    "from fastai.callback.core     import Callback\n",
    "from fastai.callback.data     import CudaCallback\n",
    "from fastai.callback.progress import ProgressCallback, ShowGraphCallback\n",
    "from fastai.callback.training import GradientClip\n",
    "from fastai.callback.rnn      import ModelResetter, RNNCallback, RNNRegularizer\n",
    "from fastai.callback.tracker  import EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# https://docs.fast.ai/callback.training.html#GradientClip\n",
    "class GradientClipping(Callback):\n",
    "    def __init__(self, clip=None): self.clip = clip\n",
    "    def after_backward(self):\n",
    "        if self.clip:  nn.utils.clip_grad_norm_(self.run.model.parameters(), self.clip)\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShowGraphEveryBatchCallback(Callback):\n",
    "    \"Update a graph of training and validation loss\"\n",
    "        \n",
    "    def after_batch(self):\n",
    "        if self.iter%15 == 0:\n",
    "            rec   = self.learn.recorder\n",
    "            iters = range(len(rec.losses))\n",
    "            x_bounds = (0, self.n_epoch * self.n_iter)\n",
    "            y_bounds = (0, 10)\n",
    "            self.progress.mbar.update_graph([(iters, rec.losses)], x_bounds, y_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls       = DataLoaders(train_dl, valid_dl),\n",
    "                model     = SimpleLSTM(vocab_sz=VOCAB_LEN, emb_dim=300, hidden_dim=700, n_layers=2), \n",
    "                loss_func = CrossEntropyLossFlat(),\n",
    "                metrics   = [accuracy, Perplexity],\n",
    "                cbs       = [\n",
    "                          #  TrainEvalCallback, # By default\n",
    "                          #  Recorder,          # By default\n",
    "                          #  ProgressCallback,  # By default\n",
    "                             CudaCallback,\n",
    "                             ModelResetter,\n",
    "                             ShowGraphEveryBatchCallback,\n",
    "                             GradientClip(max_norm=0.1),\n",
    "                          #  TensorBoardCallback(\"/home/javi/Escritorio/tensorboard\", trace_model=True)\n",
    "                          #  partial(AvgStatsCallback,accuracy_flat),\n",
    "                          #  Recorder,  #     Registers statistics (lr, loss and metrics) during training\n",
    "                          #  partial(GradientClipping, clip=0.1),\n",
    "                          #  partial(RNNTrainer, α=2., β=1.),\n",
    "                          #  ParamScheduler({'lr': SchedLin(1e-3, 1e-2)})\n",
    "                          #  EarlyStoppingCallback(patience=3)\n",
    "                          #  SaveModelCallback()\n",
    "                            ])\n",
    "#learn.show_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.lr_find()\n",
    "#fig = learn.recorder.plot(suggestion=True, return_fig=True);\n",
    "#lr  = learn.recorder.min_grad_lr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='224' class='' max='1680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      13.33% [224/1680 00:57<06:11 5.9803]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-b5a7e19d6581>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# learn.fit_one_cycle(1, 1e-2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, n_epoch, lr, wd, cbs, reset_opt)\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_hypers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_fit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelFitException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_end_cleanup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_end_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'before_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_cancel_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mfinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_do_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'epoch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelEpochException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset_opt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'before_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_cancel_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mfinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_do_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_epoch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_epoch_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_do_epoch_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_epoch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelTrainException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_epoch_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'before_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_cancel_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mfinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36mall_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mall_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36mone_batch\u001b[0;34m(self, i, b)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_one_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelBatchException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_epoch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'before_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_cancel_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mfinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_do_one_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'before_backward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'step'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelStepException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(1, 1e-3) # learn.fit_one_cycle(1, 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|experiment |epochs| train_loss| valid_loss |accuracy  | perplexity |time | notes                 |\n",
    "|-----------|------|-----------|------------|----------|------------|-----|:---------------------:|\n",
    "|     a     |  1   |19.061800  | 45.006886  | 0.090237 |     -      |09:10|          -            |\n",
    "|     b     |  1   |5.943309   | 7.154285   | 0.090238 |1279.577148\t|09:38| only predicts `xxmaj` |\n",
    "\n",
    "Objetive (in 10 epochs) [video](https://youtu.be/vnOpEwmtFJ8?t=7549): train_loss=4.03 valid_loss=4.06 valid_accuracy=0.2867"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model(batch_x.cuda()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_p = learn.model(batch_x.cuda()).cpu()\n",
    "batch_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(new_batch_p, batch_y.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7, 7, 7,  ..., 7, 7, 7],\n",
       "        [7, 7, 7,  ..., 7, 7, 7],\n",
       "        [7, 7, 7,  ..., 7, 7, 7],\n",
       "        ...,\n",
       "        [7, 7, 7,  ..., 7, 7, 7],\n",
       "        [7, 7, 7,  ..., 7, 7, 7],\n",
       "        [7, 7, 7,  ..., 7, 7, 7]], device='cuda:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = new_batch_p.argmax(dim=-1)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([7], device='cuda:0'), tensor([4480], device='cuda:0'))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.unique(return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(learn.model.state_dict(), './language_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Model: AWD-LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We divide the zeros&ones mask to **mantain the std** at applying the droput mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_mask(x, size, drop_prob):\n",
    "    return x.new(*size).bernoulli_(drop_prob).div_(drop_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., 10., 10.,  0.,  0.,  0., 10.,  0.],\n",
       "        [10.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0., 10.,  0.,  0., 10.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 10.,  0.,  0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.rand(10,10)\n",
    "mask = dropout_mask(t, (10,10), 0.1)\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `EmbeddingDropout` (aka `TokenDropout`)\n",
    "- EmbeddingDropout applies dropout to the whole embedding matrix, to make the some rows equals zeros.\n",
    "- THIS dropout is NOT APPLIEID ON THE OUTPUT of the embeding layer. TH dROP OUT IS APPLIED TO THE EMBEDDING WEITHS themselfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 2, 1],\n",
       "        [0, 3, 2, 1, 2]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.LongTensor([[0, 1, 2, 2, 1],\n",
    "                          [0, 3, 2, 1, 2]])\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6585, -0.0951, -1.0614,  0.8276, -0.3901,  1.5006, -0.8054],\n",
       "         [ 0.0229, -0.1936,  0.4089,  0.5277, -0.1738, -0.3447,  0.4222],\n",
       "         [-0.9440,  0.7588,  0.0509, -1.0544, -0.6787, -2.0373, -0.0296],\n",
       "         [-0.9440,  0.7588,  0.0509, -1.0544, -0.6787, -2.0373, -0.0296],\n",
       "         [ 0.0229, -0.1936,  0.4089,  0.5277, -0.1738, -0.3447,  0.4222]],\n",
       "\n",
       "        [[-0.6585, -0.0951, -1.0614,  0.8276, -0.3901,  1.5006, -0.8054],\n",
       "         [ 0.0906,  0.6282,  1.3173,  0.0417, -0.1491, -0.4256,  0.0710],\n",
       "         [-0.9440,  0.7588,  0.0509, -1.0544, -0.6787, -2.0373, -0.0296],\n",
       "         [ 0.0229, -0.1936,  0.4089,  0.5277, -0.1738, -0.3447,  0.4222],\n",
       "         [-0.9440,  0.7588,  0.0509, -1.0544, -0.6787, -2.0373, -0.0296]]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = nn.Embedding(num_embeddings=100, embedding_dim=7, padding_idx=99) # For padding_idx will return a vector of zeros\n",
    "out_emb = emb(input)\n",
    "out_emb # [BS, SEQ_LEN, EMB_DIM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 1., 0., 1.])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout = torch.distributions.bernoulli.Bernoulli(0.3)\n",
    "dropout.sample(sample_shape=(out_emb.shape[1], ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(out_emb.shape[1], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'Bernoulli'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-3af275b595d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBernoulli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'Bernoulli'"
     ]
    }
   ],
   "source": [
    "torch.Bernoulli(0.3).sample((out.shape[1], ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.LongTensor([[0, 1, 2, 2, 1],\n",
    "                          [0, 3, 2, 1, 2]])\n",
    "\n",
    "out = emb(input)\n",
    "rw = Bernoulli(0.3).sample((out.shape[1], ))\n",
    "\n",
    "out[:, rw==1].mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([21, 32, 49, 61, 42])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_input = torch.randint(low=0, high=100, size=(5,))\n",
    "tst_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3420, -1.4190,  0.7275, -0.6516, -2.5164,  1.8570, -1.4382,  0.4643],\n",
       "        [-0.8923, -0.3391, -0.3041, -1.3148,  1.4897,  0.1525,  1.4502,  0.0357],\n",
       "        [-0.0545, -2.0934, -1.7578, -0.4612,  1.1825, -0.5089, -0.1025, -0.1985],\n",
       "        [ 0.9805, -0.6491, -0.5569, -0.6984,  0.2961, -1.0677,  1.8033, -0.0987],\n",
       "        [ 0.4785, -1.3237, -1.3687, -0.5233, -1.0823,  0.4919, -1.1676, -0.7927]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc(tst_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = nn.Dropout2d(p=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4275, -0.0000,  0.0000, -0.8145, -3.1455,  2.3213, -1.7978,  0.5804],\n",
       "        [-0.0000, -0.4239, -0.3801, -0.0000,  1.8621,  0.1906,  0.0000,  0.0446],\n",
       "        [-0.0682, -0.0000, -0.0000, -0.5765,  1.4781, -0.0000, -0.1282, -0.2481],\n",
       "        [ 1.2256, -0.0000, -0.6961, -0.8730,  0.0000, -0.0000,  0.0000, -0.1234],\n",
       "        [ 0.5982, -1.6546, -0.0000, -0.6541, -1.3528,  0.6148, -1.4595, -0.9909]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d(enc(tst_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDropout(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb, embed_p):\n",
    "        super().__init__()\n",
    "        self.emb,self.embed_p = emb,embed_p\n",
    "        self.pad_idx = self.emb.padding_idx\n",
    "        if self.pad_idx is None: self.pad_idx = -1\n",
    " \n",
    "    def forward(self, words, scale=None):\n",
    "        if self.training and self.embed_p != 0:\n",
    "            size = (self.emb.weight.size(0),1)\n",
    "            mask = dropout_mask(self.emb.weight.data, size, self.embed_p)\n",
    "            masked_embed = self.emb.weight * mask\n",
    "        else:\n",
    "            masked_embed = self.emb.weight\n",
    "        if scale:\n",
    "            masked_embed.mul_(scale)\n",
    "        return F.embedding(words, masked_embed, self.pad_idx, self.emb.max_norm,\n",
    "                           self.emb.norm_type, self.emb.scale_grad_by_freq, self.emb.sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.functional' has no attribute 'embedding'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-86d8e50d511b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0menc_dp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbeddingDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtst_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtst_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_dp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtst_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-1bc7860daaf7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, words, scale)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mmasked_embed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         return F.embedding(words, masked_embed, self.pad_idx, self.emb.max_norm,\n\u001b[0m\u001b[1;32m     19\u001b[0m                            self.emb.norm_type, self.emb.scale_grad_by_freq, self.emb.sparse)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.functional' has no attribute 'embedding'"
     ]
    }
   ],
   "source": [
    "enc = nn.Embedding(100, 7, padding_idx=1)\n",
    "enc_dp = EmbeddingDropout(enc, 0.5)\n",
    "tst_input = torch.randint(0,100,(8,))\n",
    "tst_input, enc_dp(tst_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `RNNDropout` (aka `SequenceDropout`)\n",
    "Dropout on the entire sequence dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RNNDropout(nn.Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super().__init__()\n",
    "        self.p=p\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.training or self.p == 0.: return x\n",
    "        m = dropout_mask(x.data, (x.size(0), 1, x.size(2)), self.p)\n",
    "        return x * m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `WeightDropout`(aka `DropConnect`)\n",
    "This is dropout not on the activations but **on the weights themselfs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "WEIGHT_HH = 'weight_hh_l0'\n",
    "\n",
    "class WeightDropout(nn.Module):\n",
    "    def __init__(self, module, weight_p=[0.], layer_names=[WEIGHT_HH]):\n",
    "        super().__init__()\n",
    "        self.module,self.weight_p,self.layer_names = module,weight_p,layer_names\n",
    "        for layer in self.layer_names:\n",
    "            #Makes a copy of the weights of the selected layers.\n",
    "            w = getattr(self.module, layer)\n",
    "            self.register_parameter(f'{layer}_raw', nn.Parameter(w.data))\n",
    "            self.module._parameters[layer] = F.dropout(w, p=self.weight_p, training=False)\n",
    "\n",
    "    def _setweights(self):\n",
    "        for layer in self.layer_names:\n",
    "            raw_w = getattr(self, f'{layer}_raw')\n",
    "            self.module._parameters[layer] = F.dropout(raw_w, p=self.weight_p, training=self.training)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        self._setweights() # HERE WE APPLY DROPCONNECT\n",
    "        with warnings.catch_warnings():\n",
    "            #To avoid the warning that comes because the weights aren't flattened.\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            return self.module.forward(*args)\n",
    "        \n",
    "DropConnect = WeightDropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main model: `AWD_LSTM`\n",
    "- The main model is a regular LSTM with several layers, but using all those kinds of dropouts.\n",
    "- This is AWD-LSTM inspired by https://arxiv.org/abs/1708.02182."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    input_size: The number of expected features in the input `x`\n",
    "    hidden_size: The number of features in the hidden state `h`\n",
    "    num_layers: Number of recurrent layers. E.g., setting ``num_layers=2``\n",
    "        would mean stacking two LSTMs together to form a `stacked LSTM`,\n",
    "        with the second LSTM taking in outputs of the first LSTM and\n",
    "        computing the final results. Default: 1\n",
    "    bias\n",
    "    \n",
    "    \n",
    "[nn.LSTM(input_size  = emb_sz if l == 0 else n_hid,\n",
    "         hidden_size = (n_hid if l != n_layers - 1 else emb_sz),\n",
    "         num_layers  = 1,\n",
    "         batch_first = True) for l in range(n_layers) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AWDLSTM_body(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token,\n",
    "                 hidden_p=0.2,\n",
    "                 input_p=0.6,\n",
    "                 embed_dropout_prob  = 0.1, # aka TokenDropout\n",
    "                 weight_dropout_prob = 0.5):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.bs,self.emb_sz,self.n_hid,self.n_layers = 1,emb_sz,n_hid,n_layers\n",
    "        \n",
    "        # Embeding\n",
    "        self.emb    = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)\n",
    "        emb_init_range = 0.1\n",
    "        self.emb.weight.data.uniform_(-emb_init_range, emb_init_range)\n",
    "        \n",
    "        # LSTMs\n",
    "        self.rnns = [nn.LSTM(input_size  = emb_sz if l == 0 else n_hid,\n",
    "                             hidden_size = (n_hid if l != n_layers - 1 else emb_sz),\n",
    "                             num_layers  = 1,\n",
    "                             batch_first = True) for l in range(n_layers) ]\n",
    "        \n",
    "        # Dropouts\n",
    "        self.emb_dp = EmbeddingDropout(self.emb, embed_dropout_prob)\n",
    "        self.input_dp = RNNDropout(input_p)\n",
    "        self.rnns = nn.ModuleList([WeightDropout(rnn, weight_dropout_prob) for rnn in self.rnns])\n",
    "        self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)])\n",
    "\n",
    "    def forward(self, input):\n",
    "        bs,sl = input.size()\n",
    "        if bs!=self.bs:\n",
    "            self.bs=bs\n",
    "            self.reset()\n",
    "        raw_output = self.input_dp(self.emb_dp(input))\n",
    "        new_hidden,raw_outputs,outputs = [],[],[]\n",
    "        \n",
    "        for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):\n",
    "            raw_output, new_h = rnn(raw_output, self.hidden[l])\n",
    "            new_hidden.append(new_h)\n",
    "            raw_outputs.append(raw_output)\n",
    "            if l != self.n_layers - 1: raw_output = hid_dp(raw_output)\n",
    "            outputs.append(raw_output) \n",
    "        self.hidden = to_detach(new_hidden)\n",
    "        return raw_outputs, outputs\n",
    "    \n",
    "    def to_detach(h):\n",
    "        \"Detaches `h` from its history.\"\n",
    "        return h.detach() if type(h) == torch.Tensor else tuple(to_detach(v) for v in h)\n",
    "\n",
    "    def _one_hidden(self, l):\n",
    "        \"Return one hidden state.\"\n",
    "        nh = self.n_hid if l != self.n_layers - 1 else self.emb_sz\n",
    "        return next(self.parameters()).new(1, self.bs, nh).zero_()\n",
    "\n",
    "    def reset(self):\n",
    "        \"Reset the hidden states.\"\n",
    "        self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)]\n",
    "\n",
    "\n",
    "class AWDLSTM_linearHead(nn.Module):\n",
    "    def __init__(self, n_out, n_hid, output_p, tie_encoder=None, bias=True):\n",
    "        super().__init__()\n",
    "        self.output_dp = RNNDropout(output_p)\n",
    "        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\n",
    "        if bias: self.decoder.bias.data.zero_()\n",
    "        if tie_encoder: self.decoder.weight = tie_encoder.weight\n",
    "        else: init.kaiming_uniform_(self.decoder.weight)\n",
    "\n",
    "    def forward(self, input):\n",
    "        raw_outputs, outputs = input\n",
    "        output = self.output_dp(outputs[-1]).contiguous()\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded, raw_outputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
