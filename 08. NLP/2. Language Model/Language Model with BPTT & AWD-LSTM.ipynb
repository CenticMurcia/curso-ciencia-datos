{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: We need preprocessed texts (tokinized and numericalized) in this notebook\n",
    "\n",
    "> Notebook based on:\n",
    "> 1. https://github.com/fastai/course-v3/blob/master/nbs/dl2/12_text.ipynb\n",
    "> 2. https://github.com/fastai/course-v3/blob/master/nbs/dl2/12a_awd_lstm.ipynb\n",
    "> 3. https://github.com/fastai/course-v3/blob/master/nbs/dl2/12b_lm_pretrain.ipynb\n",
    "> 4. https://github.com/fastai/course-v3/blob/master/nbs/dl2/12c_ulmfit.ipynb\n",
    "> \n",
    "> Video:\n",
    "> - https://youtu.be/vnOpEwmtFJ8?t=4687 from 1:18:00 to 2:08:00 (50 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pathlib\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test  train  unsup  vocab.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls \"../../Datasets/NLP/IMBd_prepro\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg  pos\r\n"
     ]
    }
   ],
   "source": [
    "!ls \"../../Datasets/NLP/IMBd_prepro/train\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = open(\"../../Datasets/NLP/IMBd_prepro/vocab.pkl\",'rb')\n",
    "vocab = pickle.load(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65539"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_LEN = len(vocab)\n",
    "VOCAB_LEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "def parallel_map(func, array):\n",
    "    \n",
    "    cpu_cores = multiprocessing.cpu_count()\n",
    "    array_len = len(array)\n",
    "    chunksize = array_len // 100\n",
    "    \n",
    "    if cpu_cores<2:\n",
    "        return list(tqdm(map(func, arr), total=array_len))\n",
    "    else:\n",
    "        with ProcessPoolExecutor(max_workers=cpu_cores) as ex:\n",
    "            return list(tqdm(ex.map(func, array, chunksize=chunksize), total=array_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <center> Dataset & Dataloader for Langauge Model\n",
    "- X: Text\n",
    "- Y: Same text but shifted by 1 token\n",
    "\n",
    "At every epoch:\n",
    "\n",
    "1. **Shuffle** (sort randomly) our collection of texts.\n",
    "2. **Concatenate** the individual texts together into a big stream. \n",
    "3. **Cut** this stream into a certain number of batches (which is our batch size).\n",
    "   - For instance, if the stream has 50,000 tokens and we set a batch size of 10, this will give us 10 mini-streams of 5,000 tokens.\n",
    "   \n",
    "So to recap, at every epoch we shuffle our collection of documents and concatenate them into a stream of tokens. We then cut that stream into a batch of fixed-size consecutive mini-streams. Our model will then read the mini-streams in order, and thanks to an inner state, it will produce the same activation whatever sequence length we picked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, prepro_texts_dir, bptt=70, shuffle=False):\n",
    "        \n",
    "        # Read tokenized and numeralized text files (numpy format)\n",
    "        np_filepaths = list( pathlib.Path(prepro_texts_dir).glob('**/*.npy') ) \n",
    "        \n",
    "        # Open numpy arrays in parallel\n",
    "        self.texts_np = parallel_map(func=np.load, array=np_filepaths)\n",
    "\n",
    "        self.bptt    = bptt\n",
    "        self.shuffle = shuffle\n",
    "        self.total_tokens = sum([len(t) for t in self.texts_np])\n",
    "        \n",
    "        self.concat_texts_into_stream()\n",
    "        \n",
    "    # this is necesseary at the begining of every epoch for train !!!!\n",
    "    def concat_texts_into_stream(self):\n",
    "        \n",
    "        # 1. Reorder texts if we need to\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.texts_np)\n",
    "            #self.texts_np = self.texts_np[np.random.permutation(len(self.texts_np))]\n",
    "            \n",
    "        # 2. Concat texts into a large stream\n",
    "        self.stream = np.concatenate(self.texts_np)\n",
    "        #self.stream = torch.cat([torch.Tensor(t) for t in self.texts_np])\n",
    "                \n",
    "    def __len__(self):\n",
    "        return self.total_tokens // self.bptt\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.stream[idx   : idx+self.bptt]\n",
    "        y = self.stream[idx+1 : idx+self.bptt+1] # shifted by 1\n",
    "        \n",
    "        # convert from numpy.uint16 to torch.int64        \n",
    "        x = torch.tensor(x.astype(\"int64\"))\n",
    "        y = torch.tensor(y.astype(\"int64\"))\n",
    "        \n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57bad9ddb3b443de8da90e794a7c1893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c86ea826c343fd9208128441ddae6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BPTT_LEN = 70 # Lengh of the minisequences in the big stream\n",
    "\n",
    "train_ds = LM_Dataset(\"../../Datasets/NLP/IMBd_prepro/train\", bptt=BPTT_LEN, shuffle=True)\n",
    "valid_ds = LM_Dataset(\"../../Datasets/NLP/IMBd_prepro/test\",  bptt=BPTT_LEN, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   2,   18,  140,   12,  188,   13,  100,   58,   35,   53,   19,   29,\n",
       "           10,   30,   18,   56,  122,   16,   15, 4386,    9,    7,   54,   22,\n",
       "           37,   94,   18,   78,  157,   10,   30,    8,   29,   15,   12,  250,\n",
       "           23,   67,   29,   18,  495,    9,    7,    8,  779,   38,  339,   10,\n",
       "            8, 1361,   38,  339,   10,    8, 2423,   38,  339,   10,   11,   54,\n",
       "           38,   12,  188,   13,  172,  430,   17,    8,   29,   10]),\n",
       " tensor([  18,  140,   12,  188,   13,  100,   58,   35,   53,   19,   29,   10,\n",
       "           30,   18,   56,  122,   16,   15, 4386,    9,    7,   54,   22,   37,\n",
       "           94,   18,   78,  157,   10,   30,    8,   29,   15,   12,  250,   23,\n",
       "           67,   29,   18,  495,    9,    7,    8,  779,   38,  339,   10,    8,\n",
       "         1361,   38,  339,   10,    8, 2423,   38,  339,   10,   11,   54,   38,\n",
       "           12,  188,   13,  172,  430,   17,    8,   29,   10,  280]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([70])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\" \".join(denumericalize(ds[0][0])), \" \".join(denumericalize(ds[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader (with custom sampler for BPTT)\n",
    "\n",
    "if we divide our big stream of **28 elements** with **batch_size of 5**:\n",
    "\n",
    "|               |              |               |               |               |               |\n",
    "|---------------|--------------|---------------|---------------|---------------|---------------|\n",
    "| **1st batch** | stream_idx 0 | stream_idx 6  | stream_idx 12 | stream_idx 18 | stream_idx 23 |\n",
    "| **2nd batch** | stream_idx 1 | stream_idx 7  | stream_idx 13 | stream_idx 19 | stream_idx 24 |\n",
    "| **3rd batch** | stream_idx 2 | stream_idx 8  | stream_idx 14 | stream_idx 20 | stream_idx 25 |\n",
    "| **4th batch** | stream_idx 3 | stream_idx 8  | stream_idx 15 | stream_idx 21 | stream_idx 26 |\n",
    "| **5th batch** | stream_idx 4 | stream_idx 10 | stream_idx 16 | stream_idx 22 | stream_idx 27 |\n",
    "| **6th batch** | stream_idx 5 | stream_idx 11 | stream_idx 17 |               |               |\n",
    "\n",
    "https://pytorchnlp.readthedocs.io/en/latest/_modules/torchnlp/samplers/bptt_sampler.html\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPTT_BatchSampler(Sampler):\n",
    "    def __init__(self, n_elements, batch_size, drop_last):\n",
    "        \n",
    "        indexes = np.array_split(list(range(n_elements)), batch_size) # magic happens here\n",
    "        \n",
    "        n_batches = n_elements//batch_size\n",
    "        self.batches_idxs = np.array([x[:n_batches] for x in indexes]).T.tolist()\n",
    "        \n",
    "        if not drop_last:\n",
    "            last_batch_idxs = np.array([x[n_batches:] for x in indexes if x[n_batches:].size==1]).T.tolist()\n",
    "            self.batches_idxs += last_batch_idxs\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return iter(self.batches_idxs)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.batches_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,\n",
       " [[0, 6, 12, 18, 23],\n",
       "  [1, 7, 13, 19, 24],\n",
       "  [2, 8, 14, 20, 25],\n",
       "  [3, 9, 15, 21, 26],\n",
       "  [4, 10, 16, 22, 27],\n",
       "  [5, 11, 17]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = BPTT_BatchSampler(n_elements=28, batch_size=5, drop_last=False)\n",
    "len(s), list(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " [[0, 6, 12, 18, 23],\n",
       "  [1, 7, 13, 19, 24],\n",
       "  [2, 8, 14, 20, 25],\n",
       "  [3, 9, 15, 21, 26],\n",
       "  [4, 10, 16, 22, 27]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = BPTT_BatchSampler(n_elements=28, batch_size=5, drop_last=True)\n",
    "len(s), list(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_sampler=BPTT_BatchSampler(n_elements=len(train_ds),\n",
    "                                                                batch_size=BATCH_SIZE,\n",
    "                                                                drop_last=True))\n",
    "\n",
    "valid_dl = DataLoader(valid_ds, batch_sampler=BPTT_BatchSampler(n_elements=len(valid_ds),\n",
    "                                                                batch_size=BATCH_SIZE,\n",
    "                                                                drop_last=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 70])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x = next(iter(train_dl))[0]\n",
    "batch_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Model: Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_sz, emb_dim, hidden_dim, n_layers):\n",
    "        super().__init__()\n",
    "                \n",
    "        # Embeding\n",
    "        self.emb    = nn.Embedding(vocab_sz, emb_dim, padding_idx=PAD_TOKEN)\n",
    "        emb_init_range = 0.1\n",
    "        self.emb.weight.data.uniform_(-emb_init_range, emb_init_range)\n",
    "        \n",
    "        # LSTMs\n",
    "        self.lstm = nn.LSTM(input_size  = emb_dim,\n",
    "                            hidden_size = hidden_dim,\n",
    "                            num_layers  = n_layers,\n",
    "                            batch_first = True)\n",
    "        self.in_hidden = [torch.zeros(n_layers, BATCH_SIZE, hidden_dim) for _ in range(2)] # Initial hidden state\n",
    "        \n",
    "        # Linear\n",
    "        self.head = nn.Linear(in_features=hidden_dim, out_features=vocab_sz, bias=True)\n",
    "        \n",
    "    def forward(self, input_seq):\n",
    "        out = self.emb(input_seq)\n",
    "        out, out_hidden = self.lstm(out, self.in_hidden)\n",
    "        out = self.head(out)\n",
    "        \n",
    "        self.in_hidden = [h.detach() for h in out_hidden] # Truncated BPTT\n",
    "        return out\n",
    "    \n",
    "    def reset(self):\n",
    "        for h in self.in_hidden: h.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 70, 65539])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test model\n",
    "model = SimpleLSTM(vocab_sz=VOCAB_LEN, emb_dim=300, hidden_dim=700, n_layers=2)\n",
    "#tst_input = torch.randint(low=0,high=100, size=(64,70))\n",
    "model(batch_x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *\n",
    "from fastai.learner           import Learner\n",
    "from fastai.data.core         import DataLoaders\n",
    "from fastai.callback.progress import ProgressCallback\n",
    "from fastai.callback.data     import CudaCallback\n",
    "from fastai.callback.training import GradientClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.fast.ai/callback.training.html#GradientClip\n",
    "class GradientClipping(Callback):\n",
    "    def __init__(self, clip=None): self.clip = clip\n",
    "    def after_backward(self):\n",
    "        if self.clip:  nn.utils.clip_grad_norm_(self.run.model.parameters(), self.clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls       = DataLoaders(train_dl, valid_dl),\n",
    "                model     = SimpleLSTM(vocab_sz=VOCAB_LEN, emb_dim=300, hidden_dim=700, n_layers=2), \n",
    "                loss_func = CrossEntropyLossFlat(), \n",
    "                metrics   = accuracy,\n",
    "                cbs       = [ModelResetter,\n",
    "                             CudaCallback,\n",
    "                          #  partial(AvgStatsCallback,accuracy_flat),\n",
    "                          #  Recorder,  #     Registers statistics (lr, loss and metrics) during training\n",
    "                          #  partial(GradientClipping, clip=0.1),\n",
    "                          #  partial(RNNTrainer, α=2., β=1.),\n",
    "                             ProgressCallback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(learn.model.state_dict(), './language_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Model: AWD-LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We divide the zeros&ones mask to **mantain the std** at applying the droput mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_mask(x, size, drop_prob):\n",
    "    return x.new(*size).bernoulli_(drop_prob).div_(drop_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., 10., 10.,  0.,  0.,  0., 10.,  0.],\n",
       "        [10.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0., 10.,  0.,  0., 10.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 10.,  0.,  0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.rand(10,10)\n",
    "mask = dropout_mask(t, (10,10), 0.1)\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `EmbeddingDropout` (aka `TokenDropout`)\n",
    "- EmbeddingDropout applies dropout to the whole embedding matrix, to make the some rows equals zeros.\n",
    "- THIS dropout is NOT APPLIEID ON THE OUTPUT of the embeding layer. TH dROP OUT IS APPLIED TO THE EMBEDDING WEITHS themselfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 2, 1],\n",
       "        [0, 3, 2, 1, 2]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.LongTensor([[0, 1, 2, 2, 1],\n",
    "                          [0, 3, 2, 1, 2]])\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6585, -0.0951, -1.0614,  0.8276, -0.3901,  1.5006, -0.8054],\n",
       "         [ 0.0229, -0.1936,  0.4089,  0.5277, -0.1738, -0.3447,  0.4222],\n",
       "         [-0.9440,  0.7588,  0.0509, -1.0544, -0.6787, -2.0373, -0.0296],\n",
       "         [-0.9440,  0.7588,  0.0509, -1.0544, -0.6787, -2.0373, -0.0296],\n",
       "         [ 0.0229, -0.1936,  0.4089,  0.5277, -0.1738, -0.3447,  0.4222]],\n",
       "\n",
       "        [[-0.6585, -0.0951, -1.0614,  0.8276, -0.3901,  1.5006, -0.8054],\n",
       "         [ 0.0906,  0.6282,  1.3173,  0.0417, -0.1491, -0.4256,  0.0710],\n",
       "         [-0.9440,  0.7588,  0.0509, -1.0544, -0.6787, -2.0373, -0.0296],\n",
       "         [ 0.0229, -0.1936,  0.4089,  0.5277, -0.1738, -0.3447,  0.4222],\n",
       "         [-0.9440,  0.7588,  0.0509, -1.0544, -0.6787, -2.0373, -0.0296]]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = nn.Embedding(num_embeddings=100, embedding_dim=7, padding_idx=99) # For padding_idx will return a vector of zeros\n",
    "out_emb = emb(input)\n",
    "out_emb # [BS, SEQ_LEN, EMB_DIM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 1., 0., 1.])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout = torch.distributions.bernoulli.Bernoulli(0.3)\n",
    "dropout.sample(sample_shape=(out_emb.shape[1], ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(out_emb.shape[1], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'Bernoulli'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-3af275b595d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBernoulli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'Bernoulli'"
     ]
    }
   ],
   "source": [
    "torch.Bernoulli(0.3).sample((out.shape[1], ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.LongTensor([[0, 1, 2, 2, 1],\n",
    "                          [0, 3, 2, 1, 2]])\n",
    "\n",
    "out = emb(input)\n",
    "rw = Bernoulli(0.3).sample((out.shape[1], ))\n",
    "\n",
    "out[:, rw==1].mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([21, 32, 49, 61, 42])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_input = torch.randint(low=0, high=100, size=(5,))\n",
    "tst_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3420, -1.4190,  0.7275, -0.6516, -2.5164,  1.8570, -1.4382,  0.4643],\n",
       "        [-0.8923, -0.3391, -0.3041, -1.3148,  1.4897,  0.1525,  1.4502,  0.0357],\n",
       "        [-0.0545, -2.0934, -1.7578, -0.4612,  1.1825, -0.5089, -0.1025, -0.1985],\n",
       "        [ 0.9805, -0.6491, -0.5569, -0.6984,  0.2961, -1.0677,  1.8033, -0.0987],\n",
       "        [ 0.4785, -1.3237, -1.3687, -0.5233, -1.0823,  0.4919, -1.1676, -0.7927]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc(tst_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = nn.Dropout2d(p=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4275, -0.0000,  0.0000, -0.8145, -3.1455,  2.3213, -1.7978,  0.5804],\n",
       "        [-0.0000, -0.4239, -0.3801, -0.0000,  1.8621,  0.1906,  0.0000,  0.0446],\n",
       "        [-0.0682, -0.0000, -0.0000, -0.5765,  1.4781, -0.0000, -0.1282, -0.2481],\n",
       "        [ 1.2256, -0.0000, -0.6961, -0.8730,  0.0000, -0.0000,  0.0000, -0.1234],\n",
       "        [ 0.5982, -1.6546, -0.0000, -0.6541, -1.3528,  0.6148, -1.4595, -0.9909]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d(enc(tst_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDropout(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb, embed_p):\n",
    "        super().__init__()\n",
    "        self.emb,self.embed_p = emb,embed_p\n",
    "        self.pad_idx = self.emb.padding_idx\n",
    "        if self.pad_idx is None: self.pad_idx = -1\n",
    " \n",
    "    def forward(self, words, scale=None):\n",
    "        if self.training and self.embed_p != 0:\n",
    "            size = (self.emb.weight.size(0),1)\n",
    "            mask = dropout_mask(self.emb.weight.data, size, self.embed_p)\n",
    "            masked_embed = self.emb.weight * mask\n",
    "        else:\n",
    "            masked_embed = self.emb.weight\n",
    "        if scale:\n",
    "            masked_embed.mul_(scale)\n",
    "        return F.embedding(words, masked_embed, self.pad_idx, self.emb.max_norm,\n",
    "                           self.emb.norm_type, self.emb.scale_grad_by_freq, self.emb.sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.functional' has no attribute 'embedding'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-86d8e50d511b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0menc_dp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbeddingDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtst_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtst_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_dp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtst_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-1bc7860daaf7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, words, scale)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mmasked_embed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         return F.embedding(words, masked_embed, self.pad_idx, self.emb.max_norm,\n\u001b[0m\u001b[1;32m     19\u001b[0m                            self.emb.norm_type, self.emb.scale_grad_by_freq, self.emb.sparse)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.functional' has no attribute 'embedding'"
     ]
    }
   ],
   "source": [
    "enc = nn.Embedding(100, 7, padding_idx=1)\n",
    "enc_dp = EmbeddingDropout(enc, 0.5)\n",
    "tst_input = torch.randint(0,100,(8,))\n",
    "tst_input, enc_dp(tst_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `RNNDropout` (aka `SequenceDropout`)\n",
    "Dropout on the entire sequence dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RNNDropout(nn.Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super().__init__()\n",
    "        self.p=p\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.training or self.p == 0.: return x\n",
    "        m = dropout_mask(x.data, (x.size(0), 1, x.size(2)), self.p)\n",
    "        return x * m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `WeightDropout`(aka `DropConnect`)\n",
    "This is dropout not on the activations but **on the weights themselfs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "WEIGHT_HH = 'weight_hh_l0'\n",
    "\n",
    "class WeightDropout(nn.Module):\n",
    "    def __init__(self, module, weight_p=[0.], layer_names=[WEIGHT_HH]):\n",
    "        super().__init__()\n",
    "        self.module,self.weight_p,self.layer_names = module,weight_p,layer_names\n",
    "        for layer in self.layer_names:\n",
    "            #Makes a copy of the weights of the selected layers.\n",
    "            w = getattr(self.module, layer)\n",
    "            self.register_parameter(f'{layer}_raw', nn.Parameter(w.data))\n",
    "            self.module._parameters[layer] = F.dropout(w, p=self.weight_p, training=False)\n",
    "\n",
    "    def _setweights(self):\n",
    "        for layer in self.layer_names:\n",
    "            raw_w = getattr(self, f'{layer}_raw')\n",
    "            self.module._parameters[layer] = F.dropout(raw_w, p=self.weight_p, training=self.training)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        self._setweights() # HERE WE APPLY DROPCONNECT\n",
    "        with warnings.catch_warnings():\n",
    "            #To avoid the warning that comes because the weights aren't flattened.\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            return self.module.forward(*args)\n",
    "        \n",
    "DropConnect = WeightDropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main model: `AWD_LSTM`\n",
    "- The main model is a regular LSTM with several layers, but using all those kinds of dropouts.\n",
    "- This is AWD-LSTM inspired by https://arxiv.org/abs/1708.02182."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    input_size: The number of expected features in the input `x`\n",
    "    hidden_size: The number of features in the hidden state `h`\n",
    "    num_layers: Number of recurrent layers. E.g., setting ``num_layers=2``\n",
    "        would mean stacking two LSTMs together to form a `stacked LSTM`,\n",
    "        with the second LSTM taking in outputs of the first LSTM and\n",
    "        computing the final results. Default: 1\n",
    "    bias\n",
    "    \n",
    "    \n",
    "[nn.LSTM(input_size  = emb_sz if l == 0 else n_hid,\n",
    "         hidden_size = (n_hid if l != n_layers - 1 else emb_sz),\n",
    "         num_layers  = 1,\n",
    "         batch_first = True) for l in range(n_layers) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AWDLSTM_body(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token,\n",
    "                 hidden_p=0.2,\n",
    "                 input_p=0.6,\n",
    "                 embed_dropout_prob  = 0.1, # aka TokenDropout\n",
    "                 weight_dropout_prob = 0.5):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.bs,self.emb_sz,self.n_hid,self.n_layers = 1,emb_sz,n_hid,n_layers\n",
    "        \n",
    "        # Embeding\n",
    "        self.emb    = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)\n",
    "        emb_init_range = 0.1\n",
    "        self.emb.weight.data.uniform_(-emb_init_range, emb_init_range)\n",
    "        \n",
    "        # LSTMs\n",
    "        self.rnns = [nn.LSTM(input_size  = emb_sz if l == 0 else n_hid,\n",
    "                             hidden_size = (n_hid if l != n_layers - 1 else emb_sz),\n",
    "                             num_layers  = 1,\n",
    "                             batch_first = True) for l in range(n_layers) ]\n",
    "        \n",
    "        # Dropouts\n",
    "        self.emb_dp = EmbeddingDropout(self.emb, embed_dropout_prob)\n",
    "        self.input_dp = RNNDropout(input_p)\n",
    "        self.rnns = nn.ModuleList([WeightDropout(rnn, weight_dropout_prob) for rnn in self.rnns])\n",
    "        self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)])\n",
    "\n",
    "    def forward(self, input):\n",
    "        bs,sl = input.size()\n",
    "        if bs!=self.bs:\n",
    "            self.bs=bs\n",
    "            self.reset()\n",
    "        raw_output = self.input_dp(self.emb_dp(input))\n",
    "        new_hidden,raw_outputs,outputs = [],[],[]\n",
    "        \n",
    "        for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):\n",
    "            raw_output, new_h = rnn(raw_output, self.hidden[l])\n",
    "            new_hidden.append(new_h)\n",
    "            raw_outputs.append(raw_output)\n",
    "            if l != self.n_layers - 1: raw_output = hid_dp(raw_output)\n",
    "            outputs.append(raw_output) \n",
    "        self.hidden = to_detach(new_hidden)\n",
    "        return raw_outputs, outputs\n",
    "    \n",
    "    def to_detach(h):\n",
    "        \"Detaches `h` from its history.\"\n",
    "        return h.detach() if type(h) == torch.Tensor else tuple(to_detach(v) for v in h)\n",
    "\n",
    "    def _one_hidden(self, l):\n",
    "        \"Return one hidden state.\"\n",
    "        nh = self.n_hid if l != self.n_layers - 1 else self.emb_sz\n",
    "        return next(self.parameters()).new(1, self.bs, nh).zero_()\n",
    "\n",
    "    def reset(self):\n",
    "        \"Reset the hidden states.\"\n",
    "        self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)]\n",
    "\n",
    "\n",
    "class AWDLSTM_linearHead(nn.Module):\n",
    "    def __init__(self, n_out, n_hid, output_p, tie_encoder=None, bias=True):\n",
    "        super().__init__()\n",
    "        self.output_dp = RNNDropout(output_p)\n",
    "        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\n",
    "        if bias: self.decoder.bias.data.zero_()\n",
    "        if tie_encoder: self.decoder.weight = tie_encoder.weight\n",
    "        else: init.kaiming_uniform_(self.decoder.weight)\n",
    "\n",
    "    def forward(self, input):\n",
    "        raw_outputs, outputs = input\n",
    "        output = self.output_dp(outputs[-1]).contiguous()\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded, raw_outputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
